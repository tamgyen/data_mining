{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import seaborn_image as isns\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.8.4'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "train_data_csv = pd.read_csv('data/train.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "train_featrues, train_targets = (train_data_csv.drop(['label'], axis=1), train_data_csv.label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "train_images = train_featrues.values.astype('float32').reshape(-1, 28, 28, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 900x900 with 18 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ints = np.random.randint(0, train_images.shape[0], 9)\n",
    "_ = isns.ImageGrid([np.flipud(train_images[i, :, :, :].reshape(28, 28)) for i in ints],\n",
    "                   cbar_label=[f'{train_targets[i]}' for i in ints],\n",
    "                   col_wrap=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "train_targets = tf.keras.utils.to_categorical(train_targets, num_classes=10, dtype='float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 10)"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 28, 28, 1)"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def scale_and_augment(input):\n",
    "    x = Rescaling(1./255)(input)\n",
    "    x = RandomRotation(.042, fill_mode='constant')(x)\n",
    "    x = RandomTranslation(.25, .25, fill_mode='constant')(x)\n",
    "    x = RandomZoom((-.2, .3), fill_mode='constant')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(input, depth, kernel_size, num_kernels, activation, batchnorm):\n",
    "    for _ in range(depth):\n",
    "        x = Conv2D(filters=num_kernels,\n",
    "                   kernel_size=kernel_size,\n",
    "                   padding='same',\n",
    "                   )(input)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(activation)(x)\n",
    "\n",
    "    return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def build_cnn_1(weights=None):\n",
    "    name = 'cnn_1'\n",
    "    inputs = Input(shape=train_images.shape[1:])\n",
    "\n",
    "    x = scale_and_augment(inputs)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=64,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=128,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=256,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss=CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def build_cnn_2(weights=None):\n",
    "    name = 'cnn_2'\n",
    "    inputs = Input(shape=train_images.shape[1:])\n",
    "\n",
    "    x = scale_and_augment(inputs)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=16,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=32,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=64,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=128,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss=CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def build_cnn_3(weights=None):\n",
    "    name = 'cnn_3'\n",
    "    inputs = Input(shape=train_images.shape[1:])\n",
    "\n",
    "    x = scale_and_augment(inputs)\n",
    "\n",
    "    x1 = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=32,\n",
    "                   activation='swish',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x2 = conv_block(input=x,\n",
    "               depth=2,\n",
    "               kernel_size=5,\n",
    "               num_kernels=32,\n",
    "               activation='swish',\n",
    "               batchnorm=True)\n",
    "\n",
    "    x2 = conv_block(input=x2,\n",
    "               depth=2,\n",
    "               kernel_size=3,\n",
    "               num_kernels=32,\n",
    "               activation='swish',\n",
    "               batchnorm=True)\n",
    "\n",
    "    x3 = conv_block(input=x,\n",
    "               depth=1,\n",
    "               kernel_size=7,\n",
    "               num_kernels=32,\n",
    "               activation='swish',\n",
    "               batchnorm=True)\n",
    "\n",
    "\n",
    "    x = Concatenate()([x1, x2, x3])\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.2)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=128,\n",
    "                   activation='swish',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.2)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=256,\n",
    "                   activation='swish',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='swish')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss=CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model, name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model, name = build_cnn_3()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling (Rescaling)          (None, 28, 28, 1)    0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " random_rotation (RandomRotatio  (None, 28, 28, 1)   0           ['rescaling[0][0]']              \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " random_translation (RandomTran  (None, 28, 28, 1)   0           ['random_rotation[0][0]']        \n",
      " slation)                                                                                         \n",
      "                                                                                                  \n",
      " random_zoom (RandomZoom)       (None, 28, 28, 1)    0           ['random_translation[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 28, 28, 32)   832         ['random_zoom[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 28, 28, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 28, 28, 32)   320         ['random_zoom[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 28, 28, 32)   9248        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 28, 28, 32)   1600        ['random_zoom[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 28, 28, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 28, 28, 32)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 28, 28, 32)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 28, 28, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 28, 28, 96)   0           ['activation_2[0][0]',           \n",
      "                                                                  'activation_6[0][0]',           \n",
      "                                                                  'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 14, 14, 96)   0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 14, 14, 96)   0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 14, 14, 128)  110720      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 14, 14, 128)  512        ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 14, 14, 128)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 128)   0           ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 7, 7, 128)    0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 7, 7, 256)    295168      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 7, 7, 256)   1024        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 7, 7, 256)    0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 3, 3, 256)   0           ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 3, 3, 256)    0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2304)         0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          590080      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 256)         1024        ['dense[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           2570        ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,013,610\n",
      "Trainable params: 1,012,074\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "ds_train_set = tf.data.Dataset.from_tensor_slices((train_images, train_targets))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def brightness_augment(image, target):\n",
    "    image = tf.math.multiply(image, tf.random.uniform(shape=[], minval=.9, maxval=1))\n",
    "\n",
    "    return image, target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "SPLIT = .15\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "ds_eval = ds_train_set.take(round(train_targets.shape[0]*SPLIT))\n",
    "ds_train = ds_train_set.skip(round(train_targets.shape[0]*SPLIT))\n",
    "\n",
    "ds_train = ds_train.map(brightness_augment)\n",
    "\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_eval = ds_eval.batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_train = ds_train.cache().shuffle(2000).prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#                     ds_train,\n",
    "#                     validation_data=ds_eval,\n",
    "#                     epochs=150,\n",
    "#                     callbacks=[ModelCheckpoint(f'./models/{name}/weights_best.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)]\n",
    "#                     )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'], 'g')\n",
    "# plt.plot(history.history['val_loss'], 'r')\n",
    "# plt.title(f'{name} Loss across epochs\\n')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('# epoch')\n",
    "# plt.legend(['train', 'test'], loc='lower right')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# model.load_weights(f'./models/{name}/weights_best.h5')\n",
    "#\n",
    "# scores = model.evaluate(ds_eval)\n",
    "#\n",
    "# os.makedirs(f'./models/{name}/model', exist_ok=True)\n",
    "# model.save(filepath=f'./models/{name}/model', save_format='tf')\n",
    "#\n",
    "# print(\"done training!\")\n",
    "#\n",
    "# original_stdout = sys.stdout\n",
    "# with open(f'./models/{name}/training.info', 'w') as f:\n",
    "#     sys.stdout = f\n",
    "#\n",
    "#     print(f'\\n\\n***MODEL SUMMARY***\\nacc: {scores[2]}')\n",
    "#     print(model.summary())\n",
    "#     print('\\n\\n****FLOPS***\\n')\n",
    "#\n",
    "#     sys.stdout = original_stdout\n",
    "#\n",
    "# print('done experiment!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Ensemble + Pseudo Labelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "test_data_csv = pd.read_csv('data/test.csv')\n",
    "test_images = test_data_csv.drop(['id'], axis=1).values.astype('float32').reshape(-1, 28, 28, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "dig_data_csv = pd.read_csv('data/Dig-MNIST.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7   \n0          0       0       0       0       0       0       0       0       0  \\\n1          1       0       0       0       0       0       0       0       0   \n2          2       0       0       0       0       0       0       0       0   \n3          3       0       0       0       0       0       0       0       0   \n4          4       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n10235      5       0       0       0       0       0       0       0       0   \n10236      6       0       0       0       0       0       0       0       0   \n10237      7       0       0       0       0       0       0       0       0   \n10238      8       0       0       0       0       0       0       0       0   \n10239      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778   \n0           0  ...         0         0         0         0         0  \\\n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n10235       0  ...         0         0         0         0         0   \n10236       0  ...         0         0         0         0         0   \n10237       0  ...         0         0         0         0         0   \n10238       0  ...         0         0         0         0         0   \n10239       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n10235         0         0         0         0         0  \n10236         0         0         0         0         0  \n10237         0         0         0         0         0  \n10238         0         0         0         0         0  \n10239         0         0         0         0         0  \n\n[10240 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10235</th>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10236</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10237</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10238</th>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10239</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10240 rows  785 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig_data_csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "def train_n_models(n, dataset, prefix, split, epochs):\n",
    "    for i in range(n):\n",
    "        ds = dataset.shuffle(len(dataset), reshuffle_each_iteration=False)\n",
    "\n",
    "        SPLIT = split\n",
    "        BATCH_SIZE = 2048\n",
    "\n",
    "        ds_eval = ds.take(round(train_targets.shape[0] * SPLIT))\n",
    "        ds_train = ds.skip(round(train_targets.shape[0] * SPLIT))\n",
    "\n",
    "        ds_train = ds_train.map(brightness_augment)\n",
    "\n",
    "        ds_train = ds_train.batch(BATCH_SIZE)\n",
    "        ds_eval = ds_eval.batch(BATCH_SIZE)\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "        ds_train = ds_train.cache().shuffle(2000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        model, _ = build_cnn_3()\n",
    "\n",
    "        history = model.fit(\n",
    "            ds_train,\n",
    "            validation_data=ds_eval,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(f'./models/{prefix}/weights_best_{i}.h5', monitor='val_accuracy', save_best_only=True, mode='max',\n",
    "                                verbose=1)]\n",
    "        )\n",
    "\n",
    "        model.load_weights(f'./models/{prefix}/weights_best_{i}.h5')\n",
    "\n",
    "        os.makedirs(f'./models/{prefix}/{i}/model', exist_ok=True)\n",
    "        model.save(filepath=f'./models/{prefix}/{i}/model', save_format='tf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_n_models(n=3,\n",
    "               dataset=ds_train_set,\n",
    "               prefix='base_model',\n",
    "               split=.05,\n",
    "               epochs=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "# check\n",
    "i = 0\n",
    "model, _ = build_cnn_3(f'./models/final_models/weights_best_{i}.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "(5000, 28, 28, 1)"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "preds = model.predict(test_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "test_targets = np.argmax(preds, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "ids_confident = np.argwhere((preds > .95).any(axis=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "test_images_confident, test_targets_confident = (test_images[ids_confident].astype('float32'), np.squeeze(test_targets[ids_confident]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "test_images_confident = test_images_confident.reshape(-1, 28, 28, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "(4720, 28, 28, 1)"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_confident.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 900x900 with 18 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ints = np.random.randint(0, test_images_confident.shape[0], 9)\n",
    "_ = isns.ImageGrid([np.flipud(test_images_confident[i, :, :, :].reshape(28, 28)) for i in ints],\n",
    "                   cbar_label=[f'{test_targets_confident[i]}' for i in ints],\n",
    "                   col_wrap=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "test_targets_confident = tf.keras.utils.to_categorical(test_targets_confident, num_classes=10, dtype='float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "ds_test_confident = tf.data.Dataset.from_tensor_slices((test_images_confident, test_targets_confident))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "ds_pseudo_label = ds_train_set.concatenate(ds_test_confident)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "64720"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_pseudo_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8656 - accuracy: 0.7204\n",
      "Epoch 1: val_accuracy improved from -inf to 0.16556, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 10s 143ms/step - loss: 0.8656 - accuracy: 0.7204 - val_loss: 2.4840 - val_accuracy: 0.1656\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9297\n",
      "Epoch 2: val_accuracy improved from 0.16556 to 0.18556, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.2188 - accuracy: 0.9297 - val_loss: 3.0751 - val_accuracy: 0.1856\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9555\n",
      "Epoch 3: val_accuracy did not improve from 0.18556\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.1398 - accuracy: 0.9555 - val_loss: 3.5048 - val_accuracy: 0.1656\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9657\n",
      "Epoch 4: val_accuracy did not improve from 0.18556\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.1072 - accuracy: 0.9657 - val_loss: 3.8409 - val_accuracy: 0.1711\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9717\n",
      "Epoch 5: val_accuracy improved from 0.18556 to 0.27444, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0901 - accuracy: 0.9717 - val_loss: 3.8516 - val_accuracy: 0.2744\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9741\n",
      "Epoch 6: val_accuracy did not improve from 0.27444\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0813 - accuracy: 0.9741 - val_loss: 4.5099 - val_accuracy: 0.2078\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9768\n",
      "Epoch 7: val_accuracy did not improve from 0.27444\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0716 - accuracy: 0.9768 - val_loss: 5.0965 - val_accuracy: 0.1844\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9784\n",
      "Epoch 8: val_accuracy did not improve from 0.27444\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0664 - accuracy: 0.9784 - val_loss: 5.6879 - val_accuracy: 0.1544\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9788\n",
      "Epoch 9: val_accuracy did not improve from 0.27444\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0636 - accuracy: 0.9788 - val_loss: 4.7080 - val_accuracy: 0.1044\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9817\n",
      "Epoch 10: val_accuracy improved from 0.27444 to 0.29778, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0568 - accuracy: 0.9817 - val_loss: 2.4869 - val_accuracy: 0.2978\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9814\n",
      "Epoch 11: val_accuracy improved from 0.29778 to 0.40111, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0562 - accuracy: 0.9814 - val_loss: 1.8088 - val_accuracy: 0.4011\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9825\n",
      "Epoch 12: val_accuracy improved from 0.40111 to 0.76778, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0528 - accuracy: 0.9825 - val_loss: 0.6020 - val_accuracy: 0.7678\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9836\n",
      "Epoch 13: val_accuracy improved from 0.76778 to 0.92111, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0495 - accuracy: 0.9836 - val_loss: 0.2378 - val_accuracy: 0.9211\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9840\n",
      "Epoch 14: val_accuracy improved from 0.92111 to 0.95333, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0485 - accuracy: 0.9840 - val_loss: 0.1361 - val_accuracy: 0.9533\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9854\n",
      "Epoch 15: val_accuracy improved from 0.95333 to 0.98222, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0462 - accuracy: 0.9854 - val_loss: 0.0605 - val_accuracy: 0.9822\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9852\n",
      "Epoch 16: val_accuracy improved from 0.98222 to 0.98778, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0448 - accuracy: 0.9852 - val_loss: 0.0358 - val_accuracy: 0.9878\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9859\n",
      "Epoch 17: val_accuracy improved from 0.98778 to 0.99444, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0423 - accuracy: 0.9859 - val_loss: 0.0193 - val_accuracy: 0.9944\n",
      "Epoch 18/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9865\n",
      "Epoch 18: val_accuracy improved from 0.99444 to 0.99667, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0413 - accuracy: 0.9866 - val_loss: 0.0185 - val_accuracy: 0.9967\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9868\n",
      "Epoch 19: val_accuracy did not improve from 0.99667\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0406 - accuracy: 0.9868 - val_loss: 0.0101 - val_accuracy: 0.9956\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9864\n",
      "Epoch 20: val_accuracy did not improve from 0.99667\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0411 - accuracy: 0.9864 - val_loss: 0.0201 - val_accuracy: 0.9944\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9868\n",
      "Epoch 21: val_accuracy improved from 0.99667 to 0.99778, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0383 - accuracy: 0.9868 - val_loss: 0.0092 - val_accuracy: 0.9978\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9883\n",
      "Epoch 22: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0351 - accuracy: 0.9883 - val_loss: 0.0137 - val_accuracy: 0.9967\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9877\n",
      "Epoch 23: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.0106 - val_accuracy: 0.9944\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9886\n",
      "Epoch 24: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0342 - accuracy: 0.9886 - val_loss: 0.0142 - val_accuracy: 0.9967\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9884\n",
      "Epoch 25: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0357 - accuracy: 0.9884 - val_loss: 0.0191 - val_accuracy: 0.9956\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9885\n",
      "Epoch 26: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0359 - accuracy: 0.9885 - val_loss: 0.0101 - val_accuracy: 0.9967\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9889\n",
      "Epoch 27: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.0170 - val_accuracy: 0.9933\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9891\n",
      "Epoch 28: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0326 - accuracy: 0.9891 - val_loss: 0.0261 - val_accuracy: 0.9944\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9894\n",
      "Epoch 29: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0311 - accuracy: 0.9894 - val_loss: 0.0106 - val_accuracy: 0.9978\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9901\n",
      "Epoch 30: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0309 - accuracy: 0.9901 - val_loss: 0.0212 - val_accuracy: 0.9933\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9893\n",
      "Epoch 31: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0330 - accuracy: 0.9893 - val_loss: 0.0137 - val_accuracy: 0.9978\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9898\n",
      "Epoch 32: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0309 - accuracy: 0.9898 - val_loss: 0.0221 - val_accuracy: 0.9944\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9893\n",
      "Epoch 33: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0312 - accuracy: 0.9893 - val_loss: 0.0131 - val_accuracy: 0.9967\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9903\n",
      "Epoch 34: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0301 - accuracy: 0.9903 - val_loss: 0.0136 - val_accuracy: 0.9956\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9900\n",
      "Epoch 35: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0310 - accuracy: 0.9900 - val_loss: 0.0185 - val_accuracy: 0.9956\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9909\n",
      "Epoch 36: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0271 - accuracy: 0.9909 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9902\n",
      "Epoch 37: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0290 - accuracy: 0.9902 - val_loss: 0.0176 - val_accuracy: 0.9944\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9907\n",
      "Epoch 38: val_accuracy improved from 0.99778 to 0.99889, saving model to ./models/pseudo_label\\weights_best_0.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0277 - accuracy: 0.9907 - val_loss: 0.0168 - val_accuracy: 0.9989\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9908\n",
      "Epoch 39: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0279 - accuracy: 0.9908 - val_loss: 0.0145 - val_accuracy: 0.9989\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9920\n",
      "Epoch 40: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0259 - accuracy: 0.9920 - val_loss: 0.0158 - val_accuracy: 0.9967\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9908\n",
      "Epoch 41: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.0173 - val_accuracy: 0.9978\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9907\n",
      "Epoch 42: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.0137 - val_accuracy: 0.9967\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9912\n",
      "Epoch 43: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0272 - accuracy: 0.9912 - val_loss: 0.0177 - val_accuracy: 0.9967\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9913\n",
      "Epoch 44: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0263 - accuracy: 0.9913 - val_loss: 0.0184 - val_accuracy: 0.9967\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9917\n",
      "Epoch 45: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.0168 - val_accuracy: 0.9956\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9913\n",
      "Epoch 46: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0262 - accuracy: 0.9913 - val_loss: 0.0244 - val_accuracy: 0.9944\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9913\n",
      "Epoch 47: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0252 - accuracy: 0.9913 - val_loss: 0.0154 - val_accuracy: 0.9944\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9911\n",
      "Epoch 48: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0265 - accuracy: 0.9911 - val_loss: 0.0190 - val_accuracy: 0.9978\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9911\n",
      "Epoch 49: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0259 - accuracy: 0.9911 - val_loss: 0.0134 - val_accuracy: 0.9967\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9916\n",
      "Epoch 50: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0248 - accuracy: 0.9916 - val_loss: 0.0139 - val_accuracy: 0.9967\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9917\n",
      "Epoch 51: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.0200 - val_accuracy: 0.9956\n",
      "Epoch 52/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9917\n",
      "Epoch 52: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0255 - accuracy: 0.9918 - val_loss: 0.0152 - val_accuracy: 0.9967\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9915\n",
      "Epoch 53: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0264 - accuracy: 0.9915 - val_loss: 0.0161 - val_accuracy: 0.9967\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9924\n",
      "Epoch 54: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0243 - accuracy: 0.9924 - val_loss: 0.0247 - val_accuracy: 0.9944\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9919\n",
      "Epoch 55: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0240 - accuracy: 0.9919 - val_loss: 0.0181 - val_accuracy: 0.9967\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9917\n",
      "Epoch 56: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0241 - accuracy: 0.9917 - val_loss: 0.0162 - val_accuracy: 0.9978\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9920\n",
      "Epoch 57: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0243 - accuracy: 0.9920 - val_loss: 0.0186 - val_accuracy: 0.9944\n",
      "Epoch 58/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0237 - accuracy: 0.9922\n",
      "Epoch 58: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.0113 - val_accuracy: 0.9989\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9928\n",
      "Epoch 59: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0217 - accuracy: 0.9928 - val_loss: 0.0150 - val_accuracy: 0.9978\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9922\n",
      "Epoch 60: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.0212 - val_accuracy: 0.9944\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9924\n",
      "Epoch 61: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.0141 - val_accuracy: 0.9978\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9921\n",
      "Epoch 62: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0235 - accuracy: 0.9921 - val_loss: 0.0228 - val_accuracy: 0.9956\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9928\n",
      "Epoch 63: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.0173 - val_accuracy: 0.9967\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9925\n",
      "Epoch 64: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0218 - accuracy: 0.9925 - val_loss: 0.0151 - val_accuracy: 0.9967\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9922\n",
      "Epoch 65: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0237 - accuracy: 0.9922 - val_loss: 0.0190 - val_accuracy: 0.9956\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9927\n",
      "Epoch 66: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0224 - accuracy: 0.9927 - val_loss: 0.0246 - val_accuracy: 0.9933\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9918\n",
      "Epoch 67: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0234 - accuracy: 0.9918 - val_loss: 0.0149 - val_accuracy: 0.9956\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9927\n",
      "Epoch 68: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0221 - accuracy: 0.9927 - val_loss: 0.0220 - val_accuracy: 0.9922\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9929\n",
      "Epoch 69: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0209 - accuracy: 0.9929 - val_loss: 0.0125 - val_accuracy: 0.9978\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9933\n",
      "Epoch 70: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.0144 - val_accuracy: 0.9978\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9925\n",
      "Epoch 71: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.0128 - val_accuracy: 0.9989\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9930\n",
      "Epoch 72: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.0176 - val_accuracy: 0.9956\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9930\n",
      "Epoch 73: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0207 - accuracy: 0.9930 - val_loss: 0.0189 - val_accuracy: 0.9967\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9927\n",
      "Epoch 74: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.0145 - val_accuracy: 0.9989\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9930\n",
      "Epoch 75: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0214 - accuracy: 0.9930 - val_loss: 0.0151 - val_accuracy: 0.9978\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9931\n",
      "Epoch 76: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 0.0154 - val_accuracy: 0.9967\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9927\n",
      "Epoch 77: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0225 - accuracy: 0.9927 - val_loss: 0.0143 - val_accuracy: 0.9978\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9927\n",
      "Epoch 78: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0208 - accuracy: 0.9927 - val_loss: 0.0078 - val_accuracy: 0.9989\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9934\n",
      "Epoch 79: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0194 - accuracy: 0.9934 - val_loss: 0.0215 - val_accuracy: 0.9967\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9934\n",
      "Epoch 80: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0205 - accuracy: 0.9934 - val_loss: 0.0154 - val_accuracy: 0.9967\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9936\n",
      "Epoch 81: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0203 - accuracy: 0.9936 - val_loss: 0.0179 - val_accuracy: 0.9956\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9930\n",
      "Epoch 82: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0211 - accuracy: 0.9930 - val_loss: 0.0170 - val_accuracy: 0.9967\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9935\n",
      "Epoch 83: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0204 - accuracy: 0.9935 - val_loss: 0.0203 - val_accuracy: 0.9956\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9932\n",
      "Epoch 84: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.0174 - val_accuracy: 0.9967\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9934\n",
      "Epoch 85: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.0188 - val_accuracy: 0.9944\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9930\n",
      "Epoch 86: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 0.0180 - val_accuracy: 0.9967\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9933\n",
      "Epoch 87: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0206 - accuracy: 0.9933 - val_loss: 0.0233 - val_accuracy: 0.9978\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9937\n",
      "Epoch 88: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.0146 - val_accuracy: 0.9978\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9935\n",
      "Epoch 89: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.0203 - val_accuracy: 0.9967\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9936\n",
      "Epoch 90: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0192 - accuracy: 0.9936 - val_loss: 0.0091 - val_accuracy: 0.9989\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9934\n",
      "Epoch 91: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.0100 - val_accuracy: 0.9978\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9938\n",
      "Epoch 92: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0188 - accuracy: 0.9938 - val_loss: 0.0107 - val_accuracy: 0.9978\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9936\n",
      "Epoch 93: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.0102 - val_accuracy: 0.9978\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9930\n",
      "Epoch 94: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0203 - accuracy: 0.9930 - val_loss: 0.0106 - val_accuracy: 0.9989\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9937\n",
      "Epoch 95: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 0.0178 - val_accuracy: 0.9978\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9938\n",
      "Epoch 96: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0188 - accuracy: 0.9938 - val_loss: 0.0127 - val_accuracy: 0.9978\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9935\n",
      "Epoch 97: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0188 - accuracy: 0.9935 - val_loss: 0.0156 - val_accuracy: 0.9956\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 98: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.0151 - val_accuracy: 0.9978\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9944\n",
      "Epoch 99: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0104 - val_accuracy: 0.9978\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9936\n",
      "Epoch 100: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.0121 - val_accuracy: 0.9978\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9945\n",
      "Epoch 101: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0163 - accuracy: 0.9945 - val_loss: 0.0138 - val_accuracy: 0.9967\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9936\n",
      "Epoch 102: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.0110 - val_accuracy: 0.9967\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9936\n",
      "Epoch 103: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.0098 - val_accuracy: 0.9978\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9933\n",
      "Epoch 104: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 0.0069 - val_accuracy: 0.9967\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9935\n",
      "Epoch 105: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.0209 - val_accuracy: 0.9956\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9941\n",
      "Epoch 106: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 0.0071 - val_accuracy: 0.9989\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9943\n",
      "Epoch 107: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 0.0147 - val_accuracy: 0.9967\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9939\n",
      "Epoch 108: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.0135 - val_accuracy: 0.9967\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9938\n",
      "Epoch 109: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.0096 - val_accuracy: 0.9989\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9934\n",
      "Epoch 110: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 130ms/step - loss: 0.0198 - accuracy: 0.9934 - val_loss: 0.0190 - val_accuracy: 0.9956\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9940\n",
      "Epoch 111: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0168 - accuracy: 0.9940 - val_loss: 0.0145 - val_accuracy: 0.9978\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9941\n",
      "Epoch 112: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.0149 - val_accuracy: 0.9967\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9937\n",
      "Epoch 113: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.0181 - val_accuracy: 0.9978\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9939\n",
      "Epoch 114: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.0024 - val_accuracy: 0.9989\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9943\n",
      "Epoch 115: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.0144 - val_accuracy: 0.9956\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9939\n",
      "Epoch 116: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0123 - val_accuracy: 0.9978\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9945\n",
      "Epoch 117: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.0070 - val_accuracy: 0.9989\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9942\n",
      "Epoch 118: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.0066 - val_accuracy: 0.9989\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9946\n",
      "Epoch 119: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.0052 - val_accuracy: 0.9989\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9943\n",
      "Epoch 120: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0169 - accuracy: 0.9943 - val_loss: 0.0070 - val_accuracy: 0.9989\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9937\n",
      "Epoch 121: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.0063 - val_accuracy: 0.9978\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9942\n",
      "Epoch 122: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0163 - accuracy: 0.9942 - val_loss: 0.0079 - val_accuracy: 0.9978\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9945\n",
      "Epoch 123: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0161 - accuracy: 0.9945 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9945\n",
      "Epoch 124: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.0102 - val_accuracy: 0.9978\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9942\n",
      "Epoch 125: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0177 - accuracy: 0.9942 - val_loss: 0.0123 - val_accuracy: 0.9967\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9942\n",
      "Epoch 126: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0174 - accuracy: 0.9942 - val_loss: 0.0068 - val_accuracy: 0.9989\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9942\n",
      "Epoch 127: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0168 - accuracy: 0.9942 - val_loss: 0.0123 - val_accuracy: 0.9944\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9944\n",
      "Epoch 128: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0168 - accuracy: 0.9944 - val_loss: 0.0105 - val_accuracy: 0.9978\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 129: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.0103 - val_accuracy: 0.9956\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9945\n",
      "Epoch 130: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.0102 - val_accuracy: 0.9967\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9940\n",
      "Epoch 131: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0176 - accuracy: 0.9940 - val_loss: 0.0099 - val_accuracy: 0.9956\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9938\n",
      "Epoch 132: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.0156 - val_accuracy: 0.9978\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9939\n",
      "Epoch 133: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.0107 - val_accuracy: 0.9978\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9943\n",
      "Epoch 134: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.0089 - val_accuracy: 0.9978\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9943\n",
      "Epoch 135: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 0.0046 - val_accuracy: 0.9978\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9939\n",
      "Epoch 136: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.0117 - val_accuracy: 0.9956\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9942\n",
      "Epoch 137: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.0060 - val_accuracy: 0.9967\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9943\n",
      "Epoch 138: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.0064 - val_accuracy: 0.9978\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9947\n",
      "Epoch 139: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0158 - accuracy: 0.9947 - val_loss: 0.0088 - val_accuracy: 0.9978\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9947\n",
      "Epoch 140: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0058 - val_accuracy: 0.9978\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9947\n",
      "Epoch 141: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0161 - accuracy: 0.9947 - val_loss: 0.0052 - val_accuracy: 0.9978\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9946\n",
      "Epoch 142: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0157 - accuracy: 0.9946 - val_loss: 0.0111 - val_accuracy: 0.9978\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9945\n",
      "Epoch 143: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 0.0054 - val_accuracy: 0.9978\n",
      "Epoch 144/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9949\n",
      "Epoch 144: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 0.0082 - val_accuracy: 0.9978\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9943\n",
      "Epoch 145: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0169 - accuracy: 0.9943 - val_loss: 0.0094 - val_accuracy: 0.9978\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9950\n",
      "Epoch 146: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0149 - accuracy: 0.9950 - val_loss: 0.0088 - val_accuracy: 0.9978\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9943\n",
      "Epoch 147: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.0075 - val_accuracy: 0.9978\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9943\n",
      "Epoch 148: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.0165 - val_accuracy: 0.9967\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9950\n",
      "Epoch 149: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0072 - val_accuracy: 0.9978\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9950\n",
      "Epoch 150: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 0.0068 - val_accuracy: 0.9978\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9953\n",
      "Epoch 151: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9952\n",
      "Epoch 152: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.0092 - val_accuracy: 0.9967\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9951\n",
      "Epoch 153: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0108 - val_accuracy: 0.9978\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9951\n",
      "Epoch 154: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0141 - accuracy: 0.9951 - val_loss: 0.0110 - val_accuracy: 0.9944\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9947\n",
      "Epoch 155: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0157 - accuracy: 0.9947 - val_loss: 0.0088 - val_accuracy: 0.9956\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9952\n",
      "Epoch 156: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0168 - val_accuracy: 0.9967\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9948\n",
      "Epoch 157: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 0.0125 - val_accuracy: 0.9978\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9953\n",
      "Epoch 158: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.0130 - val_accuracy: 0.9978\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9945\n",
      "Epoch 159: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0156 - accuracy: 0.9945 - val_loss: 0.0295 - val_accuracy: 0.9911\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9942\n",
      "Epoch 160: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0162 - accuracy: 0.9942 - val_loss: 0.0219 - val_accuracy: 0.9956\n",
      "Epoch 161/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9947\n",
      "Epoch 161: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0150 - accuracy: 0.9947 - val_loss: 0.0124 - val_accuracy: 0.9956\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9948\n",
      "Epoch 162: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.0110 - val_accuracy: 0.9944\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9946\n",
      "Epoch 163: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 0.0112 - val_accuracy: 0.9978\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9946\n",
      "Epoch 164: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0150 - accuracy: 0.9946 - val_loss: 0.0054 - val_accuracy: 0.9978\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9953\n",
      "Epoch 165: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0152 - accuracy: 0.9953 - val_loss: 0.0084 - val_accuracy: 0.9978\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9949\n",
      "Epoch 166: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0145 - accuracy: 0.9949 - val_loss: 0.0162 - val_accuracy: 0.9956\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9943\n",
      "Epoch 167: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.0095 - val_accuracy: 0.9967\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9948\n",
      "Epoch 168: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.0101 - val_accuracy: 0.9956\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9957\n",
      "Epoch 169: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0158 - val_accuracy: 0.9967\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9949\n",
      "Epoch 170: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0157 - accuracy: 0.9949 - val_loss: 0.0147 - val_accuracy: 0.9967\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9949\n",
      "Epoch 171: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.0118 - val_accuracy: 0.9967\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9950\n",
      "Epoch 172: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0101 - val_accuracy: 0.9967\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9951\n",
      "Epoch 173: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0140 - accuracy: 0.9951 - val_loss: 0.0115 - val_accuracy: 0.9967\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9947\n",
      "Epoch 174: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0152 - accuracy: 0.9947 - val_loss: 0.0160 - val_accuracy: 0.9956\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9955\n",
      "Epoch 175: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.0105 - val_accuracy: 0.9978\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9945\n",
      "Epoch 176: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9950\n",
      "Epoch 177: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0142 - accuracy: 0.9950 - val_loss: 0.0082 - val_accuracy: 0.9978\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9952\n",
      "Epoch 178: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0092 - val_accuracy: 0.9956\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9952\n",
      "Epoch 179: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.0045 - val_accuracy: 0.9967\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 180: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 0.0111 - val_accuracy: 0.9956\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9951\n",
      "Epoch 181: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0142 - accuracy: 0.9951 - val_loss: 0.0107 - val_accuracy: 0.9956\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9947\n",
      "Epoch 182: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0157 - accuracy: 0.9947 - val_loss: 0.0138 - val_accuracy: 0.9956\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9953\n",
      "Epoch 183: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0137 - accuracy: 0.9953 - val_loss: 0.0187 - val_accuracy: 0.9956\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9950\n",
      "Epoch 184: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0142 - accuracy: 0.9950 - val_loss: 0.0120 - val_accuracy: 0.9956\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9947\n",
      "Epoch 185: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0137 - val_accuracy: 0.9978\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9948\n",
      "Epoch 186: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0150 - accuracy: 0.9948 - val_loss: 0.0117 - val_accuracy: 0.9956\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9952\n",
      "Epoch 187: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0077 - val_accuracy: 0.9967\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9950\n",
      "Epoch 188: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0189 - val_accuracy: 0.9933\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9951\n",
      "Epoch 189: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0155 - val_accuracy: 0.9956\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9953\n",
      "Epoch 190: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.0081 - val_accuracy: 0.9967\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9949\n",
      "Epoch 191: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0133 - val_accuracy: 0.9956\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9951\n",
      "Epoch 192: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0111 - val_accuracy: 0.9944\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9952\n",
      "Epoch 193: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0144 - accuracy: 0.9952 - val_loss: 0.0119 - val_accuracy: 0.9978\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9952\n",
      "Epoch 194: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0140 - accuracy: 0.9952 - val_loss: 0.0085 - val_accuracy: 0.9978\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9944\n",
      "Epoch 195: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0160 - accuracy: 0.9944 - val_loss: 0.0114 - val_accuracy: 0.9956\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9950\n",
      "Epoch 196: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.0090 - val_accuracy: 0.9978\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9953\n",
      "Epoch 197: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0145 - accuracy: 0.9953 - val_loss: 0.0117 - val_accuracy: 0.9944\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9947\n",
      "Epoch 198: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0146 - accuracy: 0.9947 - val_loss: 0.0127 - val_accuracy: 0.9967\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9951\n",
      "Epoch 199: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.0118 - val_accuracy: 0.9978\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9952\n",
      "Epoch 200: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.0089 - val_accuracy: 0.9967\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9953\n",
      "Epoch 201: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.0083 - val_accuracy: 0.9956\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9954\n",
      "Epoch 202: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0145 - val_accuracy: 0.9967\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9953\n",
      "Epoch 203: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0134 - val_accuracy: 0.9956\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9952\n",
      "Epoch 204: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.0092 - val_accuracy: 0.9978\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9957\n",
      "Epoch 205: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0094 - val_accuracy: 0.9956\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9952\n",
      "Epoch 206: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0143 - accuracy: 0.9952 - val_loss: 0.0143 - val_accuracy: 0.9922\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9951\n",
      "Epoch 207: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0137 - val_accuracy: 0.9967\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9955\n",
      "Epoch 208: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0135 - accuracy: 0.9955 - val_loss: 0.0123 - val_accuracy: 0.9967\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9955\n",
      "Epoch 209: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0125 - accuracy: 0.9955 - val_loss: 0.0090 - val_accuracy: 0.9967\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9951\n",
      "Epoch 210: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0140 - accuracy: 0.9951 - val_loss: 0.0100 - val_accuracy: 0.9956\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9955\n",
      "Epoch 211: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 0.0125 - val_accuracy: 0.9956\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 212: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 0.0106 - val_accuracy: 0.9978\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9950\n",
      "Epoch 213: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0146 - accuracy: 0.9950 - val_loss: 0.0087 - val_accuracy: 0.9967\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9952\n",
      "Epoch 214: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0141 - accuracy: 0.9952 - val_loss: 0.0092 - val_accuracy: 0.9967\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9952\n",
      "Epoch 215: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.0147 - val_accuracy: 0.9944\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9953\n",
      "Epoch 216: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.0134 - val_accuracy: 0.9967\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9954\n",
      "Epoch 217: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0139 - val_accuracy: 0.9944\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9956\n",
      "Epoch 218: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0133 - accuracy: 0.9956 - val_loss: 0.0054 - val_accuracy: 0.9967\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9958\n",
      "Epoch 219: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.0036 - val_accuracy: 0.9978\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9957\n",
      "Epoch 220: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0090 - val_accuracy: 0.9989\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9953\n",
      "Epoch 221: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0135 - accuracy: 0.9953 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9958\n",
      "Epoch 222: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0092 - val_accuracy: 0.9989\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9958\n",
      "Epoch 223: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0078 - val_accuracy: 0.9978\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9953\n",
      "Epoch 224: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0139 - accuracy: 0.9953 - val_loss: 0.0153 - val_accuracy: 0.9956\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9953\n",
      "Epoch 225: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.0081 - val_accuracy: 0.9978\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9956\n",
      "Epoch 226: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0127 - accuracy: 0.9956 - val_loss: 0.0100 - val_accuracy: 0.9989\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9956\n",
      "Epoch 227: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0126 - accuracy: 0.9956 - val_loss: 0.0102 - val_accuracy: 0.9956\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9956\n",
      "Epoch 228: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0118 - val_accuracy: 0.9956\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9958\n",
      "Epoch 229: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0146 - val_accuracy: 0.9967\n",
      "Epoch 230/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9956\n",
      "Epoch 230: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.0094 - val_accuracy: 0.9978\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9959\n",
      "Epoch 231: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.0096 - val_accuracy: 0.9967\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 232: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 0.0060 - val_accuracy: 0.9989\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9952\n",
      "Epoch 233: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0134 - accuracy: 0.9952 - val_loss: 0.0066 - val_accuracy: 0.9978\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 234: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 0.0120 - val_accuracy: 0.9967\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9955\n",
      "Epoch 235: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 0.0160 - val_accuracy: 0.9956\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9956\n",
      "Epoch 236: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.0124 - val_accuracy: 0.9967\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9957\n",
      "Epoch 237: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0177 - val_accuracy: 0.9978\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9959\n",
      "Epoch 238: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.0132 - val_accuracy: 0.9956\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9956\n",
      "Epoch 239: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0162 - val_accuracy: 0.9944\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9957\n",
      "Epoch 240: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0123 - accuracy: 0.9957 - val_loss: 0.0193 - val_accuracy: 0.9956\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9957\n",
      "Epoch 241: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0109 - val_accuracy: 0.9967\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9954\n",
      "Epoch 242: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0080 - val_accuracy: 0.9956\n",
      "Epoch 243/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9958\n",
      "Epoch 243: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0074 - val_accuracy: 0.9967\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9955\n",
      "Epoch 244: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0079 - val_accuracy: 0.9967\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9954\n",
      "Epoch 245: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0154 - val_accuracy: 0.9967\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 246: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 0.0090 - val_accuracy: 0.9967\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 247: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0059 - val_accuracy: 0.9978\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9955\n",
      "Epoch 248: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.0114 - val_accuracy: 0.9967\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9959\n",
      "Epoch 249: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.0194 - val_accuracy: 0.9967\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9956\n",
      "Epoch 250: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0084 - val_accuracy: 0.9956\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 251: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0148 - val_accuracy: 0.9956\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 252: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0117 - val_accuracy: 0.9967\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9958\n",
      "Epoch 253: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0166 - val_accuracy: 0.9944\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 254: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0148 - val_accuracy: 0.9944\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9957\n",
      "Epoch 255: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0190 - val_accuracy: 0.9967\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9956\n",
      "Epoch 256: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0112 - val_accuracy: 0.9978\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 257: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0075 - val_accuracy: 0.9956\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 258: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0127 - val_accuracy: 0.9956\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9952\n",
      "Epoch 259: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.0103 - val_accuracy: 0.9956\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9959\n",
      "Epoch 260: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0127 - accuracy: 0.9959 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
      "Epoch 261: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.0069 - val_accuracy: 0.9956\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9960\n",
      "Epoch 262: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.0160 - val_accuracy: 0.9967\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9956\n",
      "Epoch 263: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0209 - val_accuracy: 0.9944\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9961\n",
      "Epoch 264: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0126 - val_accuracy: 0.9956\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9957\n",
      "Epoch 265: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.0124 - val_accuracy: 0.9967\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9964\n",
      "Epoch 266: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0079 - val_accuracy: 0.9978\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9961\n",
      "Epoch 267: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9959\n",
      "Epoch 268: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0112 - accuracy: 0.9959 - val_loss: 0.0079 - val_accuracy: 0.9956\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9957\n",
      "Epoch 269: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0123 - accuracy: 0.9957 - val_loss: 0.0191 - val_accuracy: 0.9967\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9961\n",
      "Epoch 270: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0128 - val_accuracy: 0.9967\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9962\n",
      "Epoch 271: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0108 - val_accuracy: 0.9956\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
      "Epoch 272: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.0114 - val_accuracy: 0.9967\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9961\n",
      "Epoch 273: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0116 - accuracy: 0.9961 - val_loss: 0.0159 - val_accuracy: 0.9956\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 274: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0131 - val_accuracy: 0.9967\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9960\n",
      "Epoch 275: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 0.0134 - val_accuracy: 0.9967\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9962\n",
      "Epoch 276: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.0070 - val_accuracy: 0.9956\n",
      "Epoch 277/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9960\n",
      "Epoch 277: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 0.0101 - val_accuracy: 0.9956\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9958\n",
      "Epoch 278: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0115 - accuracy: 0.9958 - val_loss: 0.0087 - val_accuracy: 0.9967\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9959\n",
      "Epoch 279: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.0212 - val_accuracy: 0.9944\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9956\n",
      "Epoch 280: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0120 - accuracy: 0.9956 - val_loss: 0.0050 - val_accuracy: 0.9978\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9960\n",
      "Epoch 281: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9959\n",
      "Epoch 282: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.0136 - val_accuracy: 0.9956\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9964\n",
      "Epoch 283: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0111 - accuracy: 0.9964 - val_loss: 0.0114 - val_accuracy: 0.9967\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9960\n",
      "Epoch 284: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 0.0126 - val_accuracy: 0.9956\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9961\n",
      "Epoch 285: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.0120 - val_accuracy: 0.9967\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9960\n",
      "Epoch 286: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 0.0147 - val_accuracy: 0.9944\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9959\n",
      "Epoch 287: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.0179 - val_accuracy: 0.9956\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9960\n",
      "Epoch 288: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.0145 - val_accuracy: 0.9967\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9962\n",
      "Epoch 289: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0106 - val_accuracy: 0.9956\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9960\n",
      "Epoch 290: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 0.0167 - val_accuracy: 0.9956\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9964\n",
      "Epoch 291: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0123 - val_accuracy: 0.9956\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9962\n",
      "Epoch 292: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0065 - val_accuracy: 0.9956\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9963\n",
      "Epoch 293: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.0090 - val_accuracy: 0.9956\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9963\n",
      "Epoch 294: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.0151 - val_accuracy: 0.9967\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9954\n",
      "Epoch 295: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0107 - val_accuracy: 0.9967\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9961\n",
      "Epoch 296: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.0082 - val_accuracy: 0.9967\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9956\n",
      "Epoch 297: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.0116 - val_accuracy: 0.9944\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9963\n",
      "Epoch 298: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0132 - val_accuracy: 0.9956\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9963\n",
      "Epoch 299: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.0065 - val_accuracy: 0.9967\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9960\n",
      "Epoch 300: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.0068 - val_accuracy: 0.9967\n",
      "INFO:tensorflow:Assets written to: ./models/pseudo_label/0/model\\assets\n",
      "Epoch 1/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8804 - accuracy: 0.7164\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11333, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 10s 130ms/step - loss: 0.8804 - accuracy: 0.7164 - val_loss: 2.3895 - val_accuracy: 0.1133\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9295\n",
      "Epoch 2: val_accuracy improved from 0.11333 to 0.12222, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.2196 - accuracy: 0.9295 - val_loss: 2.7489 - val_accuracy: 0.1222\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9543\n",
      "Epoch 3: val_accuracy improved from 0.12222 to 0.20111, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.1424 - accuracy: 0.9543 - val_loss: 3.0128 - val_accuracy: 0.2011\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9643\n",
      "Epoch 4: val_accuracy did not improve from 0.20111\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.1105 - accuracy: 0.9643 - val_loss: 2.8816 - val_accuracy: 0.1189\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9704\n",
      "Epoch 5: val_accuracy did not improve from 0.20111\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0923 - accuracy: 0.9704 - val_loss: 2.8612 - val_accuracy: 0.1444\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9750\n",
      "Epoch 6: val_accuracy did not improve from 0.20111\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0786 - accuracy: 0.9750 - val_loss: 3.0838 - val_accuracy: 0.1956\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9765\n",
      "Epoch 7: val_accuracy did not improve from 0.20111\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0743 - accuracy: 0.9765 - val_loss: 2.8269 - val_accuracy: 0.1656\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9775\n",
      "Epoch 8: val_accuracy did not improve from 0.20111\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0687 - accuracy: 0.9775 - val_loss: 4.2647 - val_accuracy: 0.1033\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9806\n",
      "Epoch 9: val_accuracy improved from 0.20111 to 0.20333, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0606 - accuracy: 0.9806 - val_loss: 3.0791 - val_accuracy: 0.2033\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9809\n",
      "Epoch 10: val_accuracy improved from 0.20333 to 0.34889, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0587 - accuracy: 0.9809 - val_loss: 2.2090 - val_accuracy: 0.3489\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9813\n",
      "Epoch 11: val_accuracy did not improve from 0.34889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0583 - accuracy: 0.9813 - val_loss: 3.0221 - val_accuracy: 0.3044\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9826\n",
      "Epoch 12: val_accuracy improved from 0.34889 to 0.63000, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0527 - accuracy: 0.9826 - val_loss: 1.0752 - val_accuracy: 0.6300\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9844\n",
      "Epoch 13: val_accuracy improved from 0.63000 to 0.76778, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0493 - accuracy: 0.9844 - val_loss: 0.6129 - val_accuracy: 0.7678\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9835\n",
      "Epoch 14: val_accuracy improved from 0.76778 to 0.92000, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0490 - accuracy: 0.9835 - val_loss: 0.2330 - val_accuracy: 0.9200\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9853\n",
      "Epoch 15: val_accuracy improved from 0.92000 to 0.98333, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0459 - accuracy: 0.9853 - val_loss: 0.0482 - val_accuracy: 0.9833\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9842\n",
      "Epoch 16: val_accuracy improved from 0.98333 to 0.99000, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 125ms/step - loss: 0.0468 - accuracy: 0.9842 - val_loss: 0.0290 - val_accuracy: 0.9900\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9855\n",
      "Epoch 17: val_accuracy improved from 0.99000 to 0.99778, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0441 - accuracy: 0.9855 - val_loss: 0.0083 - val_accuracy: 0.9978\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9858\n",
      "Epoch 18: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0429 - accuracy: 0.9858 - val_loss: 0.0127 - val_accuracy: 0.9967\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9859\n",
      "Epoch 19: val_accuracy improved from 0.99778 to 0.99889, saving model to ./models/pseudo_label\\weights_best_1.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0430 - accuracy: 0.9859 - val_loss: 0.0100 - val_accuracy: 0.9989\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9869\n",
      "Epoch 20: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0402 - accuracy: 0.9869 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9871\n",
      "Epoch 21: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0382 - accuracy: 0.9871 - val_loss: 0.0091 - val_accuracy: 0.9978\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9873\n",
      "Epoch 22: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0373 - accuracy: 0.9873 - val_loss: 0.0113 - val_accuracy: 0.9956\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9880\n",
      "Epoch 23: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.0057 - val_accuracy: 0.9989\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9885\n",
      "Epoch 24: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0349 - accuracy: 0.9885 - val_loss: 0.0126 - val_accuracy: 0.9933\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9889\n",
      "Epoch 25: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0350 - accuracy: 0.9889 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9889\n",
      "Epoch 26: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.0108 - val_accuracy: 0.9967\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9884\n",
      "Epoch 27: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0351 - accuracy: 0.9884 - val_loss: 0.0247 - val_accuracy: 0.9911\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9883\n",
      "Epoch 28: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0347 - accuracy: 0.9883 - val_loss: 0.0141 - val_accuracy: 0.9956\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9883\n",
      "Epoch 29: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0358 - accuracy: 0.9883 - val_loss: 0.0068 - val_accuracy: 0.9956\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9891\n",
      "Epoch 30: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0328 - accuracy: 0.9891 - val_loss: 0.0102 - val_accuracy: 0.9956\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9897\n",
      "Epoch 31: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0313 - accuracy: 0.9897 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9899\n",
      "Epoch 32: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0304 - accuracy: 0.9899 - val_loss: 0.0242 - val_accuracy: 0.9933\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9897\n",
      "Epoch 33: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0312 - accuracy: 0.9897 - val_loss: 0.0099 - val_accuracy: 0.9956\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9900\n",
      "Epoch 34: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0304 - accuracy: 0.9900 - val_loss: 0.0108 - val_accuracy: 0.9956\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9901\n",
      "Epoch 35: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0296 - accuracy: 0.9901 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9901\n",
      "Epoch 36: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0294 - accuracy: 0.9901 - val_loss: 0.0084 - val_accuracy: 0.9978\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9900\n",
      "Epoch 37: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0300 - accuracy: 0.9900 - val_loss: 0.0072 - val_accuracy: 0.9989\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9901\n",
      "Epoch 38: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0293 - accuracy: 0.9901 - val_loss: 0.0081 - val_accuracy: 0.9989\n",
      "Epoch 39/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9909\n",
      "Epoch 39: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0282 - accuracy: 0.9910 - val_loss: 0.0270 - val_accuracy: 0.9911\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9907\n",
      "Epoch 40: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0106 - val_accuracy: 0.9956\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9910\n",
      "Epoch 41: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0279 - accuracy: 0.9910 - val_loss: 0.0218 - val_accuracy: 0.9933\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9902\n",
      "Epoch 42: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0282 - accuracy: 0.9902 - val_loss: 0.0110 - val_accuracy: 0.9989\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9905\n",
      "Epoch 43: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0281 - accuracy: 0.9905 - val_loss: 0.0061 - val_accuracy: 0.9978\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9908\n",
      "Epoch 44: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.0081 - val_accuracy: 0.9967\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9912\n",
      "Epoch 45: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0272 - accuracy: 0.9912 - val_loss: 0.0063 - val_accuracy: 0.9978\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9907\n",
      "Epoch 46: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0278 - accuracy: 0.9907 - val_loss: 0.0108 - val_accuracy: 0.9967\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9916\n",
      "Epoch 47: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0252 - accuracy: 0.9916 - val_loss: 0.0111 - val_accuracy: 0.9967\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9912\n",
      "Epoch 48: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.0142 - val_accuracy: 0.9967\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9915\n",
      "Epoch 49: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0261 - accuracy: 0.9915 - val_loss: 0.0184 - val_accuracy: 0.9933\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9913\n",
      "Epoch 50: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0263 - accuracy: 0.9913 - val_loss: 0.0148 - val_accuracy: 0.9956\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9906\n",
      "Epoch 51: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0271 - accuracy: 0.9906 - val_loss: 0.0060 - val_accuracy: 0.9978\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9913\n",
      "Epoch 52: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.0101 - val_accuracy: 0.9967\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9915\n",
      "Epoch 53: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0250 - accuracy: 0.9915 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9919\n",
      "Epoch 54: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0252 - accuracy: 0.9919 - val_loss: 0.0049 - val_accuracy: 0.9989\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9916\n",
      "Epoch 55: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0247 - accuracy: 0.9916 - val_loss: 0.0134 - val_accuracy: 0.9956\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9913\n",
      "Epoch 56: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0261 - accuracy: 0.9913 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9925\n",
      "Epoch 57: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0232 - accuracy: 0.9925 - val_loss: 0.0140 - val_accuracy: 0.9922\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9912\n",
      "Epoch 58: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0261 - accuracy: 0.9912 - val_loss: 0.0036 - val_accuracy: 0.9989\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9913\n",
      "Epoch 59: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0255 - accuracy: 0.9913 - val_loss: 0.0062 - val_accuracy: 0.9978\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9923\n",
      "Epoch 60: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.0088 - val_accuracy: 0.9978\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9920\n",
      "Epoch 61: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0236 - accuracy: 0.9920 - val_loss: 0.0049 - val_accuracy: 0.9989\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9924\n",
      "Epoch 62: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0075 - val_accuracy: 0.9978\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9924\n",
      "Epoch 63: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0228 - accuracy: 0.9924 - val_loss: 0.0117 - val_accuracy: 0.9967\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9920\n",
      "Epoch 64: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 0.0090 - val_accuracy: 0.9978\n",
      "Epoch 65/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9925\n",
      "Epoch 65: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 0.0078 - val_accuracy: 0.9956\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9915\n",
      "Epoch 66: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0244 - accuracy: 0.9915 - val_loss: 0.0137 - val_accuracy: 0.9944\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9923\n",
      "Epoch 67: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.0135 - val_accuracy: 0.9956\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9930\n",
      "Epoch 68: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0211 - accuracy: 0.9930 - val_loss: 0.0125 - val_accuracy: 0.9967\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9930\n",
      "Epoch 69: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.0084 - val_accuracy: 0.9967\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9922\n",
      "Epoch 70: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0231 - accuracy: 0.9922 - val_loss: 0.0063 - val_accuracy: 0.9978\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9931\n",
      "Epoch 71: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0221 - accuracy: 0.9931 - val_loss: 0.0134 - val_accuracy: 0.9956\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9924\n",
      "Epoch 72: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0213 - accuracy: 0.9924 - val_loss: 0.0062 - val_accuracy: 0.9978\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9924\n",
      "Epoch 73: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0225 - accuracy: 0.9924 - val_loss: 0.0139 - val_accuracy: 0.9956\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9928\n",
      "Epoch 74: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 0.0141 - val_accuracy: 0.9956\n",
      "Epoch 75/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0203 - accuracy: 0.9935\n",
      "Epoch 75: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 0.0126 - val_accuracy: 0.9967\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9933\n",
      "Epoch 76: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.0123 - val_accuracy: 0.9956\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9925\n",
      "Epoch 77: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0217 - accuracy: 0.9925 - val_loss: 0.0180 - val_accuracy: 0.9956\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9930\n",
      "Epoch 78: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0206 - accuracy: 0.9930 - val_loss: 0.0075 - val_accuracy: 0.9978\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9927\n",
      "Epoch 79: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.0109 - val_accuracy: 0.9967\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9934\n",
      "Epoch 80: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0205 - accuracy: 0.9934 - val_loss: 0.0119 - val_accuracy: 0.9978\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9932\n",
      "Epoch 81: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 0.0140 - val_accuracy: 0.9978\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9932\n",
      "Epoch 82: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.0175 - val_accuracy: 0.9956\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9931\n",
      "Epoch 83: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0199 - accuracy: 0.9931 - val_loss: 0.0088 - val_accuracy: 0.9967\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9938\n",
      "Epoch 84: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.0190 - val_accuracy: 0.9944\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9932\n",
      "Epoch 85: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0200 - accuracy: 0.9932 - val_loss: 0.0069 - val_accuracy: 0.9967\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9934\n",
      "Epoch 86: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0202 - accuracy: 0.9934 - val_loss: 0.0117 - val_accuracy: 0.9944\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9929\n",
      "Epoch 87: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0211 - accuracy: 0.9929 - val_loss: 0.0075 - val_accuracy: 0.9967\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9935\n",
      "Epoch 88: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.0121 - val_accuracy: 0.9956\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9937\n",
      "Epoch 89: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.0076 - val_accuracy: 0.9967\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9937\n",
      "Epoch 90: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.0058 - val_accuracy: 0.9978\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9936\n",
      "Epoch 91: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 0.0122 - val_accuracy: 0.9944\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9932\n",
      "Epoch 92: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0200 - accuracy: 0.9932 - val_loss: 0.0144 - val_accuracy: 0.9933\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9933\n",
      "Epoch 93: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0193 - accuracy: 0.9933 - val_loss: 0.0062 - val_accuracy: 0.9989\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9937\n",
      "Epoch 94: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0184 - accuracy: 0.9937 - val_loss: 0.0100 - val_accuracy: 0.9967\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9933\n",
      "Epoch 95: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0200 - accuracy: 0.9933 - val_loss: 0.0073 - val_accuracy: 0.9978\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9937\n",
      "Epoch 96: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0193 - accuracy: 0.9937 - val_loss: 0.0209 - val_accuracy: 0.9944\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9936\n",
      "Epoch 97: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0202 - accuracy: 0.9936 - val_loss: 0.0108 - val_accuracy: 0.9967\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9936\n",
      "Epoch 98: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0195 - accuracy: 0.9936 - val_loss: 0.0118 - val_accuracy: 0.9967\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9937\n",
      "Epoch 99: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0186 - accuracy: 0.9937 - val_loss: 0.0101 - val_accuracy: 0.9978\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9940\n",
      "Epoch 100: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.0110 - val_accuracy: 0.9967\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9935\n",
      "Epoch 101: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 0.0134 - val_accuracy: 0.9944\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9937\n",
      "Epoch 102: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0185 - accuracy: 0.9937 - val_loss: 0.0079 - val_accuracy: 0.9956\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9933\n",
      "Epoch 103: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.0048 - val_accuracy: 0.9978\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9935\n",
      "Epoch 104: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.0050 - val_accuracy: 0.9978\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9943\n",
      "Epoch 105: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 0.0095 - val_accuracy: 0.9956\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9941\n",
      "Epoch 106: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0179 - accuracy: 0.9941 - val_loss: 0.0061 - val_accuracy: 0.9967\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9939\n",
      "Epoch 107: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0114 - val_accuracy: 0.9956\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9936\n",
      "Epoch 108: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 0.0183 - accuracy: 0.9936 - val_loss: 0.0098 - val_accuracy: 0.9978\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9938\n",
      "Epoch 109: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0183 - accuracy: 0.9938 - val_loss: 0.0094 - val_accuracy: 0.9967\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9937\n",
      "Epoch 110: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0172 - accuracy: 0.9937 - val_loss: 0.0078 - val_accuracy: 0.9978\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9941\n",
      "Epoch 111: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.0115 - val_accuracy: 0.9967\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9943\n",
      "Epoch 112: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.0164 - val_accuracy: 0.9944\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9941\n",
      "Epoch 113: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9944\n",
      "Epoch 114: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0164 - accuracy: 0.9944 - val_loss: 0.0068 - val_accuracy: 0.9967\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9940\n",
      "Epoch 115: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.0062 - val_accuracy: 0.9978\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944\n",
      "Epoch 116: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0033 - val_accuracy: 0.9978\n",
      "Epoch 117/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0178 - accuracy: 0.9941\n",
      "Epoch 117: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0177 - accuracy: 0.9941 - val_loss: 0.0058 - val_accuracy: 0.9978\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9936\n",
      "Epoch 118: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0191 - accuracy: 0.9936 - val_loss: 0.0119 - val_accuracy: 0.9967\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9939\n",
      "Epoch 119: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944\n",
      "Epoch 120: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0136 - val_accuracy: 0.9967\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9937\n",
      "Epoch 121: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0174 - accuracy: 0.9937 - val_loss: 0.0156 - val_accuracy: 0.9956\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9936\n",
      "Epoch 122: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0188 - accuracy: 0.9936 - val_loss: 0.0055 - val_accuracy: 0.9978\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9937\n",
      "Epoch 123: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0178 - accuracy: 0.9937 - val_loss: 0.0092 - val_accuracy: 0.9978\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9942\n",
      "Epoch 124: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.0081 - val_accuracy: 0.9978\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9940\n",
      "Epoch 125: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0169 - accuracy: 0.9940 - val_loss: 0.0127 - val_accuracy: 0.9933\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9943\n",
      "Epoch 126: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0175 - accuracy: 0.9943 - val_loss: 0.0127 - val_accuracy: 0.9956\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9945\n",
      "Epoch 127: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.0044 - val_accuracy: 0.9989\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9944\n",
      "Epoch 128: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0045 - val_accuracy: 0.9989\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944\n",
      "Epoch 129: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0114 - val_accuracy: 0.9944\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9939\n",
      "Epoch 130: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.0087 - val_accuracy: 0.9967\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9942\n",
      "Epoch 131: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.0099 - val_accuracy: 0.9978\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9943\n",
      "Epoch 132: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.0079 - val_accuracy: 0.9978\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9945\n",
      "Epoch 133: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.0064 - val_accuracy: 0.9967\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9945\n",
      "Epoch 134: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.0205 - val_accuracy: 0.9933\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9944\n",
      "Epoch 135: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.0135 - val_accuracy: 0.9956\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9942\n",
      "Epoch 136: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0169 - accuracy: 0.9942 - val_loss: 0.0072 - val_accuracy: 0.9967\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9942\n",
      "Epoch 137: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0164 - accuracy: 0.9942 - val_loss: 0.0128 - val_accuracy: 0.9956\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9942\n",
      "Epoch 138: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0170 - accuracy: 0.9942 - val_loss: 0.0090 - val_accuracy: 0.9956\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9943\n",
      "Epoch 139: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0163 - accuracy: 0.9943 - val_loss: 0.0059 - val_accuracy: 0.9989\n",
      "Epoch 140/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0166 - accuracy: 0.9947\n",
      "Epoch 140: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.0082 - val_accuracy: 0.9967\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 141: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.0105 - val_accuracy: 0.9967\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9942\n",
      "Epoch 142: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0163 - accuracy: 0.9942 - val_loss: 0.0110 - val_accuracy: 0.9967\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9948\n",
      "Epoch 143: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0153 - accuracy: 0.9948 - val_loss: 0.0123 - val_accuracy: 0.9944\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9948\n",
      "Epoch 144: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.0125 - val_accuracy: 0.9967\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9949\n",
      "Epoch 145: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.0137 - val_accuracy: 0.9944\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9951\n",
      "Epoch 146: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.0111 - val_accuracy: 0.9944\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9944\n",
      "Epoch 147: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0117 - val_accuracy: 0.9956\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9943\n",
      "Epoch 148: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0168 - accuracy: 0.9943 - val_loss: 0.0191 - val_accuracy: 0.9933\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9947\n",
      "Epoch 149: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0158 - val_accuracy: 0.9956\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9945\n",
      "Epoch 150: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.0152 - val_accuracy: 0.9967\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9948\n",
      "Epoch 151: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.0175 - val_accuracy: 0.9956\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9945\n",
      "Epoch 152: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.0210 - val_accuracy: 0.9956\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9950\n",
      "Epoch 153: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.0123 - val_accuracy: 0.9944\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9945\n",
      "Epoch 154: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.0240 - val_accuracy: 0.9944\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9948\n",
      "Epoch 155: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0149 - accuracy: 0.9948 - val_loss: 0.0039 - val_accuracy: 0.9989\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 156: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.0095 - val_accuracy: 0.9956\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9945\n",
      "Epoch 157: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0161 - accuracy: 0.9945 - val_loss: 0.0113 - val_accuracy: 0.9944\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9944\n",
      "Epoch 158: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0163 - accuracy: 0.9944 - val_loss: 0.0090 - val_accuracy: 0.9956\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9944\n",
      "Epoch 159: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0097 - val_accuracy: 0.9967\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9950\n",
      "Epoch 160: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0092 - val_accuracy: 0.9978\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9945\n",
      "Epoch 161: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.0156 - val_accuracy: 0.9956\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9945\n",
      "Epoch 162: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0156 - accuracy: 0.9945 - val_loss: 0.0192 - val_accuracy: 0.9956\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9952\n",
      "Epoch 163: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.0047 - val_accuracy: 0.9967\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9946\n",
      "Epoch 164: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0159 - accuracy: 0.9946 - val_loss: 0.0063 - val_accuracy: 0.9989\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9952\n",
      "Epoch 165: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0152 - accuracy: 0.9952 - val_loss: 0.0041 - val_accuracy: 0.9989\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9950\n",
      "Epoch 166: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0141 - accuracy: 0.9950 - val_loss: 0.0071 - val_accuracy: 0.9978\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9954\n",
      "Epoch 167: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0088 - val_accuracy: 0.9967\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9950\n",
      "Epoch 168: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 0.0120 - val_accuracy: 0.9956\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9946\n",
      "Epoch 169: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 0.0120 - val_accuracy: 0.9944\n",
      "Epoch 170/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9947\n",
      "Epoch 170: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0154 - accuracy: 0.9947 - val_loss: 0.0080 - val_accuracy: 0.9967\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9948\n",
      "Epoch 171: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.0121 - val_accuracy: 0.9956\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9949\n",
      "Epoch 172: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 0.0059 - val_accuracy: 0.9978\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9948\n",
      "Epoch 173: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 0.0083 - val_accuracy: 0.9967\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9947\n",
      "Epoch 174: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0161 - accuracy: 0.9947 - val_loss: 0.0081 - val_accuracy: 0.9978\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9954\n",
      "Epoch 175: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0048 - val_accuracy: 0.9989\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9954\n",
      "Epoch 176: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0095 - val_accuracy: 0.9967\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 177: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0072 - val_accuracy: 0.9989\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9954\n",
      "Epoch 178: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 132ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0091 - val_accuracy: 0.9978\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9952\n",
      "Epoch 179: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 128ms/step - loss: 0.0143 - accuracy: 0.9952 - val_loss: 0.0076 - val_accuracy: 0.9978\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 180: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.0074 - val_accuracy: 0.9967\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9951\n",
      "Epoch 181: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.0087 - val_accuracy: 0.9967\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9953\n",
      "Epoch 182: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.0042 - val_accuracy: 0.9989\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9952\n",
      "Epoch 183: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0149 - accuracy: 0.9952 - val_loss: 0.0151 - val_accuracy: 0.9978\n",
      "Epoch 184/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9954\n",
      "Epoch 184: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0081 - val_accuracy: 0.9956\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9950\n",
      "Epoch 185: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0146 - accuracy: 0.9950 - val_loss: 0.0033 - val_accuracy: 0.9989\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9950\n",
      "Epoch 186: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0140 - accuracy: 0.9950 - val_loss: 0.0091 - val_accuracy: 0.9967\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9951\n",
      "Epoch 187: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.0061 - val_accuracy: 0.9967\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9950\n",
      "Epoch 188: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 0.0095 - val_accuracy: 0.9967\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9950\n",
      "Epoch 189: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 125ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.0093 - val_accuracy: 0.9978\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9956\n",
      "Epoch 190: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0132 - accuracy: 0.9956 - val_loss: 0.0058 - val_accuracy: 0.9978\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9955\n",
      "Epoch 191: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.0017 - val_accuracy: 0.9989\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9947\n",
      "Epoch 192: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.0064 - val_accuracy: 0.9978\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9949\n",
      "Epoch 193: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0149 - accuracy: 0.9949 - val_loss: 0.0050 - val_accuracy: 0.9978\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9953\n",
      "Epoch 194: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0059 - val_accuracy: 0.9978\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 195: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 0.0066 - val_accuracy: 0.9978\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9950\n",
      "Epoch 196: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.0051 - val_accuracy: 0.9978\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9951\n",
      "Epoch 197: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0142 - accuracy: 0.9951 - val_loss: 0.0066 - val_accuracy: 0.9967\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9955\n",
      "Epoch 198: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 0.0108 - val_accuracy: 0.9967\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9953\n",
      "Epoch 199: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0123 - val_accuracy: 0.9956\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
      "Epoch 200: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.0058 - val_accuracy: 0.9967\n",
      "Epoch 201/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9953\n",
      "Epoch 201: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0151 - accuracy: 0.9953 - val_loss: 0.0090 - val_accuracy: 0.9956\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9952\n",
      "Epoch 202: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.0119 - val_accuracy: 0.9944\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9954\n",
      "Epoch 203: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.0133 - val_accuracy: 0.9956\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9956\n",
      "Epoch 204: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0133 - accuracy: 0.9956 - val_loss: 0.0106 - val_accuracy: 0.9956\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9957\n",
      "Epoch 205: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.0070 - val_accuracy: 0.9956\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9951\n",
      "Epoch 206: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 0.0063 - val_accuracy: 0.9978\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9960\n",
      "Epoch 207: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.0073 - val_accuracy: 0.9967\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 208: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 0.0067 - val_accuracy: 0.9967\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9952\n",
      "Epoch 209: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0137 - accuracy: 0.9952 - val_loss: 0.0049 - val_accuracy: 0.9967\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9955\n",
      "Epoch 210: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0039 - val_accuracy: 0.9989\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9958\n",
      "Epoch 211: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0122 - accuracy: 0.9958 - val_loss: 0.0036 - val_accuracy: 0.9978\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9955\n",
      "Epoch 212: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0128 - accuracy: 0.9955 - val_loss: 0.0049 - val_accuracy: 0.9978\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9957\n",
      "Epoch 213: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.0104 - val_accuracy: 0.9967\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9955\n",
      "Epoch 214: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9955\n",
      "Epoch 215: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0083 - val_accuracy: 0.9956\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9958\n",
      "Epoch 216: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0048 - val_accuracy: 0.9978\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9955\n",
      "Epoch 217: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.0090 - val_accuracy: 0.9967\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9953\n",
      "Epoch 218: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.0055 - val_accuracy: 0.9989\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9955\n",
      "Epoch 219: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0076 - val_accuracy: 0.9967\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9952\n",
      "Epoch 220: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.0065 - val_accuracy: 0.9978\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9952\n",
      "Epoch 221: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0137 - accuracy: 0.9952 - val_loss: 0.0041 - val_accuracy: 0.9978\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9955\n",
      "Epoch 222: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0052 - val_accuracy: 0.9978\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9954\n",
      "Epoch 223: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0045 - val_accuracy: 0.9978\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9961\n",
      "Epoch 224: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.0075 - val_accuracy: 0.9967\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9959\n",
      "Epoch 225: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.0056 - val_accuracy: 0.9978\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9953\n",
      "Epoch 226: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0133 - accuracy: 0.9953 - val_loss: 0.0029 - val_accuracy: 0.9989\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9957\n",
      "Epoch 227: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0030 - val_accuracy: 0.9989\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 228: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0071 - val_accuracy: 0.9967\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9958\n",
      "Epoch 229: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.0121 - val_accuracy: 0.9956\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9956\n",
      "Epoch 230: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0123 - accuracy: 0.9956 - val_loss: 0.0037 - val_accuracy: 0.9989\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 231: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0153 - val_accuracy: 0.9967\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9955\n",
      "Epoch 232: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0128 - accuracy: 0.9955 - val_loss: 0.0073 - val_accuracy: 0.9967\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
      "Epoch 233: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.0052 - val_accuracy: 0.9967\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9958\n",
      "Epoch 234: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0127 - accuracy: 0.9958 - val_loss: 0.0195 - val_accuracy: 0.9944\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9958\n",
      "Epoch 235: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.0097 - val_accuracy: 0.9956\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9954\n",
      "Epoch 236: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0120 - val_accuracy: 0.9944\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9956\n",
      "Epoch 237: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.0072 - val_accuracy: 0.9967\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9957\n",
      "Epoch 238: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.0150 - val_accuracy: 0.9967\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9963\n",
      "Epoch 239: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0086 - val_accuracy: 0.9978\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 240: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0067 - val_accuracy: 0.9956\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9956\n",
      "Epoch 241: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0123 - accuracy: 0.9956 - val_loss: 0.0174 - val_accuracy: 0.9956\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9960\n",
      "Epoch 242: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0077 - val_accuracy: 0.9978\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9951\n",
      "Epoch 243: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 0.0073 - val_accuracy: 0.9956\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9958\n",
      "Epoch 244: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0134 - val_accuracy: 0.9956\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9955\n",
      "Epoch 245: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 0.0043 - val_accuracy: 0.9989\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9960\n",
      "Epoch 246: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0072 - val_accuracy: 0.9967\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9959\n",
      "Epoch 247: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.0159 - val_accuracy: 0.9944\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9961\n",
      "Epoch 248: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.0109 - val_accuracy: 0.9967\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9957\n",
      "Epoch 249: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.0122 - val_accuracy: 0.9944\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9954\n",
      "Epoch 250: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0047 - val_accuracy: 0.9978\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9960\n",
      "Epoch 251: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.0134 - val_accuracy: 0.9944\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9956\n",
      "Epoch 252: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0127 - accuracy: 0.9956 - val_loss: 0.0131 - val_accuracy: 0.9944\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9956\n",
      "Epoch 253: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0099 - val_accuracy: 0.9944\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9961\n",
      "Epoch 254: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0113 - accuracy: 0.9961 - val_loss: 0.0163 - val_accuracy: 0.9933\n",
      "Epoch 255/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9964\n",
      "Epoch 255: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0115 - accuracy: 0.9964 - val_loss: 0.0097 - val_accuracy: 0.9967\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9958\n",
      "Epoch 256: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0146 - val_accuracy: 0.9967\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9956\n",
      "Epoch 257: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 125ms/step - loss: 0.0126 - accuracy: 0.9956 - val_loss: 0.0095 - val_accuracy: 0.9967\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9958\n",
      "Epoch 258: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 127ms/step - loss: 0.0116 - accuracy: 0.9958 - val_loss: 0.0067 - val_accuracy: 0.9978\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9957\n",
      "Epoch 259: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 0.0047 - val_accuracy: 0.9978\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9959\n",
      "Epoch 260: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.0063 - val_accuracy: 0.9956\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9962\n",
      "Epoch 261: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9955\n",
      "Epoch 262: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.0086 - val_accuracy: 0.9978\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9957\n",
      "Epoch 263: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 0.0130 - val_accuracy: 0.9956\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 264: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0161 - val_accuracy: 0.9967\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9959\n",
      "Epoch 265: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.0108 - val_accuracy: 0.9967\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9962\n",
      "Epoch 266: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0105 - val_accuracy: 0.9956\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9958\n",
      "Epoch 267: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 0.0090 - val_accuracy: 0.9967\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9959\n",
      "Epoch 268: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0119 - accuracy: 0.9959 - val_loss: 0.0084 - val_accuracy: 0.9978\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9962\n",
      "Epoch 269: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 0.0101 - val_accuracy: 0.9956\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9963\n",
      "Epoch 270: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0075 - val_accuracy: 0.9967\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9958\n",
      "Epoch 271: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0113 - accuracy: 0.9958 - val_loss: 0.0053 - val_accuracy: 0.9967\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9960\n",
      "Epoch 272: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0121 - accuracy: 0.9960 - val_loss: 0.0046 - val_accuracy: 0.9978\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9955\n",
      "Epoch 273: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0127 - accuracy: 0.9955 - val_loss: 0.0095 - val_accuracy: 0.9967\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9956\n",
      "Epoch 274: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0123 - accuracy: 0.9956 - val_loss: 0.0120 - val_accuracy: 0.9944\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9960\n",
      "Epoch 275: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0115 - accuracy: 0.9960 - val_loss: 0.0167 - val_accuracy: 0.9967\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 276: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0095 - val_accuracy: 0.9944\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9964\n",
      "Epoch 277: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0118 - val_accuracy: 0.9944\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9956\n",
      "Epoch 278: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0132 - accuracy: 0.9956 - val_loss: 0.0148 - val_accuracy: 0.9956\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
      "Epoch 279: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.0133 - val_accuracy: 0.9967\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9959\n",
      "Epoch 280: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0080 - val_accuracy: 0.9956\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9961\n",
      "Epoch 281: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.0065 - val_accuracy: 0.9978\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9961\n",
      "Epoch 282: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.0064 - val_accuracy: 0.9967\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9960\n",
      "Epoch 283: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0116 - accuracy: 0.9960 - val_loss: 0.0092 - val_accuracy: 0.9967\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9962\n",
      "Epoch 284: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 0.0123 - val_accuracy: 0.9956\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9958\n",
      "Epoch 285: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0116 - accuracy: 0.9958 - val_loss: 0.0093 - val_accuracy: 0.9967\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9964\n",
      "Epoch 286: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0109 - accuracy: 0.9964 - val_loss: 0.0144 - val_accuracy: 0.9956\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9959\n",
      "Epoch 287: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0119 - accuracy: 0.9959 - val_loss: 0.0192 - val_accuracy: 0.9956\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9963\n",
      "Epoch 288: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.0060 - val_accuracy: 0.9978\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9959\n",
      "Epoch 289: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 0.0110 - val_accuracy: 0.9956\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9962\n",
      "Epoch 290: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.0080 - val_accuracy: 0.9967\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9963\n",
      "Epoch 291: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0060 - val_accuracy: 0.9967\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9959\n",
      "Epoch 292: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0117 - accuracy: 0.9959 - val_loss: 0.0165 - val_accuracy: 0.9944\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9960\n",
      "Epoch 293: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0116 - accuracy: 0.9960 - val_loss: 0.0108 - val_accuracy: 0.9944\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9962\n",
      "Epoch 294: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 126ms/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 0.0098 - val_accuracy: 0.9956\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9963\n",
      "Epoch 295: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0110 - accuracy: 0.9963 - val_loss: 0.0117 - val_accuracy: 0.9944\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9966\n",
      "Epoch 296: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0081 - val_accuracy: 0.9967\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9959\n",
      "Epoch 297: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 126ms/step - loss: 0.0118 - accuracy: 0.9959 - val_loss: 0.0193 - val_accuracy: 0.9944\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 298: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 131ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.0118 - val_accuracy: 0.9956\n",
      "Epoch 299/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9961\n",
      "Epoch 299: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.0089 - val_accuracy: 0.9956\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9957\n",
      "Epoch 300: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0121 - accuracy: 0.9957 - val_loss: 0.0085 - val_accuracy: 0.9956\n",
      "INFO:tensorflow:Assets written to: ./models/pseudo_label/1/model\\assets\n",
      "Epoch 1/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8456 - accuracy: 0.7235\n",
      "Epoch 1: val_accuracy improved from -inf to 0.10111, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 10s 131ms/step - loss: 0.8456 - accuracy: 0.7235 - val_loss: 2.3809 - val_accuracy: 0.1011\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2112 - accuracy: 0.9326\n",
      "Epoch 2: val_accuracy improved from 0.10111 to 0.10333, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 130ms/step - loss: 0.2112 - accuracy: 0.9326 - val_loss: 2.7543 - val_accuracy: 0.1033\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9561\n",
      "Epoch 3: val_accuracy improved from 0.10333 to 0.10444, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 129ms/step - loss: 0.1358 - accuracy: 0.9561 - val_loss: 3.0661 - val_accuracy: 0.1044\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9656\n",
      "Epoch 4: val_accuracy improved from 0.10444 to 0.12667, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 128ms/step - loss: 0.1076 - accuracy: 0.9656 - val_loss: 3.1952 - val_accuracy: 0.1267\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9712\n",
      "Epoch 5: val_accuracy improved from 0.12667 to 0.15000, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 126ms/step - loss: 0.0889 - accuracy: 0.9712 - val_loss: 3.4448 - val_accuracy: 0.1500\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9742\n",
      "Epoch 6: val_accuracy improved from 0.15000 to 0.19778, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 126ms/step - loss: 0.0791 - accuracy: 0.9742 - val_loss: 3.9660 - val_accuracy: 0.1978\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9766\n",
      "Epoch 7: val_accuracy did not improve from 0.19778\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0711 - accuracy: 0.9766 - val_loss: 5.1829 - val_accuracy: 0.1178\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9788\n",
      "Epoch 8: val_accuracy did not improve from 0.19778\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0650 - accuracy: 0.9788 - val_loss: 5.0634 - val_accuracy: 0.1422\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9799\n",
      "Epoch 9: val_accuracy did not improve from 0.19778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0611 - accuracy: 0.9799 - val_loss: 4.5213 - val_accuracy: 0.1244\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9817\n",
      "Epoch 10: val_accuracy did not improve from 0.19778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0566 - accuracy: 0.9817 - val_loss: 3.7731 - val_accuracy: 0.1956\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9823\n",
      "Epoch 11: val_accuracy improved from 0.19778 to 0.48889, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0560 - accuracy: 0.9823 - val_loss: 1.5549 - val_accuracy: 0.4889\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9834\n",
      "Epoch 12: val_accuracy improved from 0.48889 to 0.57556, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0526 - accuracy: 0.9834 - val_loss: 1.2992 - val_accuracy: 0.5756\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9843\n",
      "Epoch 13: val_accuracy improved from 0.57556 to 0.85667, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0494 - accuracy: 0.9843 - val_loss: 0.4297 - val_accuracy: 0.8567\n",
      "Epoch 14/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0478 - accuracy: 0.9850\n",
      "Epoch 14: val_accuracy improved from 0.85667 to 0.92889, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0479 - accuracy: 0.9849 - val_loss: 0.2252 - val_accuracy: 0.9289\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9841\n",
      "Epoch 15: val_accuracy improved from 0.92889 to 0.98333, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 129ms/step - loss: 0.0472 - accuracy: 0.9841 - val_loss: 0.0437 - val_accuracy: 0.9833\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9853\n",
      "Epoch 16: val_accuracy improved from 0.98333 to 0.99111, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0447 - accuracy: 0.9853 - val_loss: 0.0222 - val_accuracy: 0.9911\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9856\n",
      "Epoch 17: val_accuracy improved from 0.99111 to 0.99667, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0436 - accuracy: 0.9856 - val_loss: 0.0118 - val_accuracy: 0.9967\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9858\n",
      "Epoch 18: val_accuracy improved from 0.99667 to 0.99778, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0435 - accuracy: 0.9858 - val_loss: 0.0101 - val_accuracy: 0.9978\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9865\n",
      "Epoch 19: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0409 - accuracy: 0.9865 - val_loss: 0.0085 - val_accuracy: 0.9967\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9872\n",
      "Epoch 20: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0380 - accuracy: 0.9872 - val_loss: 0.0137 - val_accuracy: 0.9944\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9877\n",
      "Epoch 21: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0385 - accuracy: 0.9877 - val_loss: 0.0144 - val_accuracy: 0.9956\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9868\n",
      "Epoch 22: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0403 - accuracy: 0.9868 - val_loss: 0.0096 - val_accuracy: 0.9967\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9883\n",
      "Epoch 23: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0363 - accuracy: 0.9883 - val_loss: 0.0136 - val_accuracy: 0.9944\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9883\n",
      "Epoch 24: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0366 - accuracy: 0.9883 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9887\n",
      "Epoch 25: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0344 - accuracy: 0.9887 - val_loss: 0.0098 - val_accuracy: 0.9956\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9883\n",
      "Epoch 26: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0367 - accuracy: 0.9883 - val_loss: 0.0091 - val_accuracy: 0.9978\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9891\n",
      "Epoch 27: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0338 - accuracy: 0.9891 - val_loss: 0.0144 - val_accuracy: 0.9956\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9892\n",
      "Epoch 28: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0325 - accuracy: 0.9892 - val_loss: 0.0121 - val_accuracy: 0.9956\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9891\n",
      "Epoch 29: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0323 - accuracy: 0.9891 - val_loss: 0.0082 - val_accuracy: 0.9956\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9898\n",
      "Epoch 30: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 0.0138 - val_accuracy: 0.9967\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9888\n",
      "Epoch 31: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0337 - accuracy: 0.9888 - val_loss: 0.0176 - val_accuracy: 0.9933\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9888\n",
      "Epoch 32: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0334 - accuracy: 0.9888 - val_loss: 0.0093 - val_accuracy: 0.9978\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9889\n",
      "Epoch 33: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0324 - accuracy: 0.9889 - val_loss: 0.0126 - val_accuracy: 0.9967\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9892\n",
      "Epoch 34: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0322 - accuracy: 0.9892 - val_loss: 0.0108 - val_accuracy: 0.9978\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9897\n",
      "Epoch 35: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0302 - accuracy: 0.9897 - val_loss: 0.0081 - val_accuracy: 0.9956\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9901\n",
      "Epoch 36: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.0144 - val_accuracy: 0.9956\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9904\n",
      "Epoch 37: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0288 - accuracy: 0.9904 - val_loss: 0.0061 - val_accuracy: 0.9967\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9904\n",
      "Epoch 38: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0294 - accuracy: 0.9904 - val_loss: 0.0125 - val_accuracy: 0.9944\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9910\n",
      "Epoch 39: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0266 - accuracy: 0.9910 - val_loss: 0.0086 - val_accuracy: 0.9967\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9905\n",
      "Epoch 40: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0278 - accuracy: 0.9905 - val_loss: 0.0116 - val_accuracy: 0.9944\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9909\n",
      "Epoch 41: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0275 - accuracy: 0.9909 - val_loss: 0.0226 - val_accuracy: 0.9933\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9911\n",
      "Epoch 42: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0265 - accuracy: 0.9911 - val_loss: 0.0163 - val_accuracy: 0.9944\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9905\n",
      "Epoch 43: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.0073 - val_accuracy: 0.9967\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9900\n",
      "Epoch 44: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0303 - accuracy: 0.9900 - val_loss: 0.0218 - val_accuracy: 0.9933\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9911\n",
      "Epoch 45: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0277 - accuracy: 0.9911 - val_loss: 0.0103 - val_accuracy: 0.9933\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9913\n",
      "Epoch 46: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0268 - accuracy: 0.9913 - val_loss: 0.0137 - val_accuracy: 0.9967\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9918\n",
      "Epoch 47: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0253 - accuracy: 0.9918 - val_loss: 0.0144 - val_accuracy: 0.9956\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9914\n",
      "Epoch 48: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0263 - accuracy: 0.9914 - val_loss: 0.0089 - val_accuracy: 0.9978\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9911\n",
      "Epoch 49: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0273 - accuracy: 0.9911 - val_loss: 0.0133 - val_accuracy: 0.9967\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9914\n",
      "Epoch 50: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0253 - accuracy: 0.9914 - val_loss: 0.0083 - val_accuracy: 0.9956\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9912\n",
      "Epoch 51: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 0.0071 - val_accuracy: 0.9967\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9918\n",
      "Epoch 52: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0256 - accuracy: 0.9918 - val_loss: 0.0123 - val_accuracy: 0.9944\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9916\n",
      "Epoch 53: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0248 - accuracy: 0.9916 - val_loss: 0.0145 - val_accuracy: 0.9933\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9916\n",
      "Epoch 54: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0251 - accuracy: 0.9916 - val_loss: 0.0149 - val_accuracy: 0.9956\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9916\n",
      "Epoch 55: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0242 - accuracy: 0.9916 - val_loss: 0.0090 - val_accuracy: 0.9956\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9911\n",
      "Epoch 56: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9919\n",
      "Epoch 57: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0240 - accuracy: 0.9919 - val_loss: 0.0085 - val_accuracy: 0.9967\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9923\n",
      "Epoch 58: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0231 - accuracy: 0.9923 - val_loss: 0.0085 - val_accuracy: 0.9967\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9920\n",
      "Epoch 59: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0239 - accuracy: 0.9920 - val_loss: 0.0113 - val_accuracy: 0.9944\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9927\n",
      "Epoch 60: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.0126 - val_accuracy: 0.9956\n",
      "Epoch 61/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9927\n",
      "Epoch 61: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.0115 - val_accuracy: 0.9933\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9924\n",
      "Epoch 62: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0225 - accuracy: 0.9924 - val_loss: 0.0183 - val_accuracy: 0.9944\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9923\n",
      "Epoch 63: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.0119 - val_accuracy: 0.9956\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9924\n",
      "Epoch 64: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.0080 - val_accuracy: 0.9944\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9922\n",
      "Epoch 65: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.0187 - val_accuracy: 0.9933\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9926\n",
      "Epoch 66: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0225 - accuracy: 0.9926 - val_loss: 0.0236 - val_accuracy: 0.9933\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9931\n",
      "Epoch 67: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.0189 - val_accuracy: 0.9944\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9920\n",
      "Epoch 68: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0136 - val_accuracy: 0.9944\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9926\n",
      "Epoch 69: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0224 - accuracy: 0.9926 - val_loss: 0.0131 - val_accuracy: 0.9944\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9927\n",
      "Epoch 70: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.0140 - val_accuracy: 0.9956\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9922\n",
      "Epoch 71: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.0091 - val_accuracy: 0.9967\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9926\n",
      "Epoch 72: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0233 - accuracy: 0.9926 - val_loss: 0.0045 - val_accuracy: 0.9967\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9926\n",
      "Epoch 73: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0219 - accuracy: 0.9926 - val_loss: 0.0069 - val_accuracy: 0.9978\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9929\n",
      "Epoch 74: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0208 - accuracy: 0.9929 - val_loss: 0.0087 - val_accuracy: 0.9967\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9932\n",
      "Epoch 75: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0215 - accuracy: 0.9932 - val_loss: 0.0128 - val_accuracy: 0.9944\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9927\n",
      "Epoch 76: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0227 - accuracy: 0.9927 - val_loss: 0.0174 - val_accuracy: 0.9956\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9932\n",
      "Epoch 77: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9938\n",
      "Epoch 78: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0196 - accuracy: 0.9938 - val_loss: 0.0056 - val_accuracy: 0.9978\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9932\n",
      "Epoch 79: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0047 - val_accuracy: 0.9978\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9930\n",
      "Epoch 80: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0205 - accuracy: 0.9930 - val_loss: 0.0059 - val_accuracy: 0.9978\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9935\n",
      "Epoch 81: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.0148 - val_accuracy: 0.9967\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9935\n",
      "Epoch 82: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.0099 - val_accuracy: 0.9944\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9935\n",
      "Epoch 83: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.0117 - val_accuracy: 0.9956\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9932\n",
      "Epoch 84: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.0104 - val_accuracy: 0.9956\n",
      "Epoch 85/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9931\n",
      "Epoch 85: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.0051 - val_accuracy: 0.9978\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9930\n",
      "Epoch 86: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0211 - accuracy: 0.9930 - val_loss: 0.0070 - val_accuracy: 0.9978\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9931\n",
      "Epoch 87: val_accuracy did not improve from 0.99778\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.0205 - val_accuracy: 0.9922\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9938\n",
      "Epoch 88: val_accuracy improved from 0.99778 to 0.99889, saving model to ./models/pseudo_label\\weights_best_2.h5\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0183 - accuracy: 0.9938 - val_loss: 0.0046 - val_accuracy: 0.9989\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9931\n",
      "Epoch 89: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 0.0162 - val_accuracy: 0.9944\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9939\n",
      "Epoch 90: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.0073 - val_accuracy: 0.9967\n",
      "Epoch 91/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9931\n",
      "Epoch 91: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0204 - accuracy: 0.9931 - val_loss: 0.0079 - val_accuracy: 0.9967\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928\n",
      "Epoch 92: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 0.0121 - val_accuracy: 0.9944\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9936\n",
      "Epoch 93: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0039 - val_accuracy: 0.9989\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9936\n",
      "Epoch 94: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.0092 - val_accuracy: 0.9956\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9938\n",
      "Epoch 95: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0182 - accuracy: 0.9938 - val_loss: 0.0147 - val_accuracy: 0.9922\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9939\n",
      "Epoch 96: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0073 - val_accuracy: 0.9967\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9935\n",
      "Epoch 97: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0195 - accuracy: 0.9935 - val_loss: 0.0130 - val_accuracy: 0.9944\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9943\n",
      "Epoch 98: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0175 - accuracy: 0.9943 - val_loss: 0.0112 - val_accuracy: 0.9956\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9937\n",
      "Epoch 99: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0200 - accuracy: 0.9937 - val_loss: 0.0234 - val_accuracy: 0.9922\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9937\n",
      "Epoch 100: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0187 - accuracy: 0.9937 - val_loss: 0.0200 - val_accuracy: 0.9944\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9937\n",
      "Epoch 101: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0187 - accuracy: 0.9937 - val_loss: 0.0180 - val_accuracy: 0.9967\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9942\n",
      "Epoch 102: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.0141 - val_accuracy: 0.9956\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9934\n",
      "Epoch 103: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0192 - accuracy: 0.9934 - val_loss: 0.0114 - val_accuracy: 0.9967\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9938\n",
      "Epoch 104: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0187 - accuracy: 0.9938 - val_loss: 0.0104 - val_accuracy: 0.9989\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9937\n",
      "Epoch 105: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.0080 - val_accuracy: 0.9978\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9935\n",
      "Epoch 106: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0187 - accuracy: 0.9935 - val_loss: 0.0083 - val_accuracy: 0.9956\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9941\n",
      "Epoch 107: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0176 - accuracy: 0.9941 - val_loss: 0.0126 - val_accuracy: 0.9944\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9940\n",
      "Epoch 108: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0178 - accuracy: 0.9940 - val_loss: 0.0071 - val_accuracy: 0.9978\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9940\n",
      "Epoch 109: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0175 - accuracy: 0.9940 - val_loss: 0.0081 - val_accuracy: 0.9978\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9938\n",
      "Epoch 110: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0185 - accuracy: 0.9938 - val_loss: 0.0079 - val_accuracy: 0.9967\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9941\n",
      "Epoch 111: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0180 - accuracy: 0.9941 - val_loss: 0.0141 - val_accuracy: 0.9956\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 112: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.0076 - val_accuracy: 0.9978\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9939\n",
      "Epoch 113: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.0111 - val_accuracy: 0.9978\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9940\n",
      "Epoch 114: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0173 - accuracy: 0.9940 - val_loss: 0.0078 - val_accuracy: 0.9978\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9939\n",
      "Epoch 115: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9935\n",
      "Epoch 116: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0187 - accuracy: 0.9935 - val_loss: 0.0099 - val_accuracy: 0.9967\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9937\n",
      "Epoch 117: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.0129 - val_accuracy: 0.9967\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9942\n",
      "Epoch 118: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0170 - accuracy: 0.9942 - val_loss: 0.0169 - val_accuracy: 0.9933\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9937\n",
      "Epoch 119: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.0197 - val_accuracy: 0.9922\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9942\n",
      "Epoch 120: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.0193 - val_accuracy: 0.9956\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9946\n",
      "Epoch 121: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0161 - accuracy: 0.9946 - val_loss: 0.0105 - val_accuracy: 0.9956\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9936\n",
      "Epoch 122: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0183 - accuracy: 0.9936 - val_loss: 0.0187 - val_accuracy: 0.9944\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9942\n",
      "Epoch 123: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0168 - accuracy: 0.9942 - val_loss: 0.0178 - val_accuracy: 0.9956\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9944\n",
      "Epoch 124: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0168 - accuracy: 0.9944 - val_loss: 0.0121 - val_accuracy: 0.9944\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9939\n",
      "Epoch 125: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0263 - val_accuracy: 0.9922\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9938\n",
      "Epoch 126: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0179 - accuracy: 0.9938 - val_loss: 0.0226 - val_accuracy: 0.9922\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9945\n",
      "Epoch 127: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0162 - accuracy: 0.9945 - val_loss: 0.0183 - val_accuracy: 0.9933\n",
      "Epoch 128/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9942\n",
      "Epoch 128: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0165 - accuracy: 0.9942 - val_loss: 0.0169 - val_accuracy: 0.9933\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9940\n",
      "Epoch 129: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0171 - accuracy: 0.9940 - val_loss: 0.0162 - val_accuracy: 0.9944\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9943\n",
      "Epoch 130: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0176 - accuracy: 0.9943 - val_loss: 0.0145 - val_accuracy: 0.9956\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9945\n",
      "Epoch 131: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 0.0251 - val_accuracy: 0.9956\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9946\n",
      "Epoch 132: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.0088 - val_accuracy: 0.9967\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9943\n",
      "Epoch 133: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0169 - accuracy: 0.9943 - val_loss: 0.0182 - val_accuracy: 0.9978\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9947\n",
      "Epoch 134: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0112 - val_accuracy: 0.9967\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9946\n",
      "Epoch 135: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.0107 - val_accuracy: 0.9967\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9949\n",
      "Epoch 136: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.0087 - val_accuracy: 0.9967\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9944\n",
      "Epoch 137: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0165 - accuracy: 0.9944 - val_loss: 0.0122 - val_accuracy: 0.9967\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9950\n",
      "Epoch 138: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0104 - val_accuracy: 0.9967\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9944\n",
      "Epoch 139: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0139 - val_accuracy: 0.9967\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9945\n",
      "Epoch 140: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0162 - accuracy: 0.9945 - val_loss: 0.0159 - val_accuracy: 0.9944\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9947\n",
      "Epoch 141: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 0.0129 - val_accuracy: 0.9967\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9946\n",
      "Epoch 142: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.0120 - val_accuracy: 0.9978\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9941\n",
      "Epoch 143: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0161 - accuracy: 0.9941 - val_loss: 0.0127 - val_accuracy: 0.9956\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9950\n",
      "Epoch 144: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0091 - val_accuracy: 0.9978\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9948\n",
      "Epoch 145: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0153 - accuracy: 0.9948 - val_loss: 0.0101 - val_accuracy: 0.9944\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9943\n",
      "Epoch 146: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.0125 - val_accuracy: 0.9956\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9943\n",
      "Epoch 147: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.0092 - val_accuracy: 0.9978\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9941\n",
      "Epoch 148: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0172 - accuracy: 0.9941 - val_loss: 0.0130 - val_accuracy: 0.9978\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9939\n",
      "Epoch 149: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0098 - val_accuracy: 0.9967\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9945\n",
      "Epoch 150: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 134ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.0120 - val_accuracy: 0.9978\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9945\n",
      "Epoch 151: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.0141 - val_accuracy: 0.9967\n",
      "Epoch 152/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9946\n",
      "Epoch 152: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0154 - accuracy: 0.9946 - val_loss: 0.0152 - val_accuracy: 0.9956\n",
      "Epoch 153/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9947\n",
      "Epoch 153: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0157 - accuracy: 0.9947 - val_loss: 0.0123 - val_accuracy: 0.9967\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9948\n",
      "Epoch 154: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0148 - accuracy: 0.9948 - val_loss: 0.0079 - val_accuracy: 0.9989\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9947\n",
      "Epoch 155: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 0.0176 - val_accuracy: 0.9956\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9941\n",
      "Epoch 156: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.0146 - val_accuracy: 0.9978\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9945\n",
      "Epoch 157: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 0.0066 - val_accuracy: 0.9978\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9948\n",
      "Epoch 158: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.0142 - val_accuracy: 0.9967\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 159: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0159 - accuracy: 0.9947 - val_loss: 0.0066 - val_accuracy: 0.9978\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9949\n",
      "Epoch 160: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.0097 - val_accuracy: 0.9978\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9949\n",
      "Epoch 161: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.0117 - val_accuracy: 0.9978\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9945\n",
      "Epoch 162: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0158 - accuracy: 0.9945 - val_loss: 0.0102 - val_accuracy: 0.9978\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9948\n",
      "Epoch 163: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0149 - accuracy: 0.9948 - val_loss: 0.0089 - val_accuracy: 0.9956\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9950\n",
      "Epoch 164: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.0103 - val_accuracy: 0.9967\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9948\n",
      "Epoch 165: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.0101 - val_accuracy: 0.9978\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9945\n",
      "Epoch 166: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0159 - accuracy: 0.9945 - val_loss: 0.0091 - val_accuracy: 0.9978\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.9946\n",
      "Epoch 167: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0157 - accuracy: 0.9946 - val_loss: 0.0074 - val_accuracy: 0.9978\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9950\n",
      "Epoch 168: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.0183 - val_accuracy: 0.9967\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9952\n",
      "Epoch 169: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.0129 - val_accuracy: 0.9967\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9947\n",
      "Epoch 170: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0154 - accuracy: 0.9947 - val_loss: 0.0338 - val_accuracy: 0.9911\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9948\n",
      "Epoch 171: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0159 - val_accuracy: 0.9956\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9948\n",
      "Epoch 172: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0153 - accuracy: 0.9948 - val_loss: 0.0112 - val_accuracy: 0.9978\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9950\n",
      "Epoch 173: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0145 - accuracy: 0.9950 - val_loss: 0.0130 - val_accuracy: 0.9956\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944\n",
      "Epoch 174: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 0.0090 - val_accuracy: 0.9967\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9952\n",
      "Epoch 175: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.0099 - val_accuracy: 0.9978\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9950\n",
      "Epoch 176: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0136 - val_accuracy: 0.9944\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9948\n",
      "Epoch 177: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0148 - accuracy: 0.9948 - val_loss: 0.0095 - val_accuracy: 0.9956\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9952\n",
      "Epoch 178: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.0090 - val_accuracy: 0.9989\n",
      "Epoch 179/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9954\n",
      "Epoch 179: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0109 - val_accuracy: 0.9978\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9951\n",
      "Epoch 180: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.0122 - val_accuracy: 0.9967\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9950\n",
      "Epoch 181: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0155 - val_accuracy: 0.9933\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9948\n",
      "Epoch 182: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0148 - accuracy: 0.9948 - val_loss: 0.0102 - val_accuracy: 0.9978\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9955\n",
      "Epoch 183: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 0.0204 - val_accuracy: 0.9944\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9949\n",
      "Epoch 184: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0153 - accuracy: 0.9949 - val_loss: 0.0110 - val_accuracy: 0.9978\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9953\n",
      "Epoch 185: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.0151 - val_accuracy: 0.9956\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9951\n",
      "Epoch 186: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0149 - accuracy: 0.9951 - val_loss: 0.0180 - val_accuracy: 0.9967\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9951\n",
      "Epoch 187: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0112 - val_accuracy: 0.9978\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9947\n",
      "Epoch 188: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0150 - accuracy: 0.9947 - val_loss: 0.0204 - val_accuracy: 0.9956\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9948\n",
      "Epoch 189: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0148 - accuracy: 0.9948 - val_loss: 0.0140 - val_accuracy: 0.9978\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9954\n",
      "Epoch 190: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0180 - val_accuracy: 0.9956\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9950\n",
      "Epoch 191: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0128 - val_accuracy: 0.9978\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9955\n",
      "Epoch 192: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.0091 - val_accuracy: 0.9967\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9951\n",
      "Epoch 193: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.0097 - val_accuracy: 0.9967\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9947\n",
      "Epoch 194: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.0198 - val_accuracy: 0.9967\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9954\n",
      "Epoch 195: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0180 - val_accuracy: 0.9967\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9952\n",
      "Epoch 196: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.0148 - val_accuracy: 0.9978\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9956\n",
      "Epoch 197: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0108 - val_accuracy: 0.9956\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9951\n",
      "Epoch 198: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0149 - accuracy: 0.9951 - val_loss: 0.0113 - val_accuracy: 0.9956\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9949\n",
      "Epoch 199: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0148 - accuracy: 0.9949 - val_loss: 0.0078 - val_accuracy: 0.9967\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9958\n",
      "Epoch 200: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0132 - accuracy: 0.9958 - val_loss: 0.0068 - val_accuracy: 0.9978\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9948\n",
      "Epoch 201: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0147 - accuracy: 0.9948 - val_loss: 0.0082 - val_accuracy: 0.9967\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9956\n",
      "Epoch 202: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.0095 - val_accuracy: 0.9967\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9950\n",
      "Epoch 203: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.0106 - val_accuracy: 0.9956\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9958\n",
      "Epoch 204: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0135 - val_accuracy: 0.9956\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9954\n",
      "Epoch 205: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0192 - val_accuracy: 0.9933\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9954\n",
      "Epoch 206: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0266 - val_accuracy: 0.9922\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9953\n",
      "Epoch 207: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0101 - val_accuracy: 0.9944\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9955\n",
      "Epoch 208: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.0086 - val_accuracy: 0.9967\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9959\n",
      "Epoch 209: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.0122 - val_accuracy: 0.9967\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9952\n",
      "Epoch 210: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0135 - accuracy: 0.9952 - val_loss: 0.0247 - val_accuracy: 0.9878\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9958\n",
      "Epoch 211: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0132 - accuracy: 0.9958 - val_loss: 0.0092 - val_accuracy: 0.9967\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9949\n",
      "Epoch 212: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0140 - val_accuracy: 0.9956\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9952\n",
      "Epoch 213: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0141 - accuracy: 0.9952 - val_loss: 0.0139 - val_accuracy: 0.9944\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9953\n",
      "Epoch 214: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.0071 - val_accuracy: 0.9956\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9956\n",
      "Epoch 215: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0131 - accuracy: 0.9956 - val_loss: 0.0113 - val_accuracy: 0.9967\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9954\n",
      "Epoch 216: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.0222 - val_accuracy: 0.9933\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9957\n",
      "Epoch 217: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0127 - val_accuracy: 0.9933\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9950\n",
      "Epoch 218: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 0.0125 - val_accuracy: 0.9956\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9955\n",
      "Epoch 219: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0128 - accuracy: 0.9955 - val_loss: 0.0178 - val_accuracy: 0.9956\n",
      "Epoch 220/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9957\n",
      "Epoch 220: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0096 - val_accuracy: 0.9956\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 221: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0060 - val_accuracy: 0.9978\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9957\n",
      "Epoch 222: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.0055 - val_accuracy: 0.9989\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9952\n",
      "Epoch 223: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0144 - accuracy: 0.9952 - val_loss: 0.0371 - val_accuracy: 0.9844\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 224: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 0.0128 - val_accuracy: 0.9944\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9958\n",
      "Epoch 225: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0049 - val_accuracy: 0.9978\n",
      "Epoch 226/300\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9955\n",
      "Epoch 226: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0104 - val_accuracy: 0.9956\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9955\n",
      "Epoch 227: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 0.0139 - val_accuracy: 0.9967\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9954\n",
      "Epoch 228: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0094 - val_accuracy: 0.9978\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9951\n",
      "Epoch 229: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 0.0142 - val_accuracy: 0.9956\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9954\n",
      "Epoch 230: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 130ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0082 - val_accuracy: 0.9967\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9957\n",
      "Epoch 231: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0059 - val_accuracy: 0.9967\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9959\n",
      "Epoch 232: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0129 - accuracy: 0.9959 - val_loss: 0.0170 - val_accuracy: 0.9967\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9953\n",
      "Epoch 233: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.0172 - val_accuracy: 0.9956\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9952\n",
      "Epoch 234: val_accuracy did not improve from 0.99889\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.0138 - val_accuracy: 0.9967\n",
      "Epoch 235/300\n",
      " 6/32 [====>.........................] - ETA: 2s - loss: 0.0147 - accuracy: 0.9959"
     ]
    }
   ],
   "source": [
    "train_n_models(n=3,\n",
    "               dataset=ds_pseudo_label,\n",
    "               prefix='pseudo_label',\n",
    "               split=.015,\n",
    "               epochs=300)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "model_0 = keras.models.load_model('C:/KBData/01_PROJECT/00_GIT/ELTE/data_mining/comp_3/models/pseudo_label/0/model')\n",
    "model_1 = keras.models.load_model('C:/KBData/01_PROJECT/00_GIT/ELTE/data_mining/comp_3/models/pseudo_label/1/model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "preds_0 = model_0.predict(test_images)\n",
    "preds_1 = model_0.predict(test_images)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
