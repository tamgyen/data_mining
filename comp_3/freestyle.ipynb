{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import seaborn_image as isns\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.8.4'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "train_data_csv = pd.read_csv('data/train.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_featrues, train_targets = (train_data_csv.drop(['label'], axis=1), train_data_csv.label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "train_images = train_featrues.values.astype('float32').reshape(-1, 28, 28, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 900x900 with 18 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ints = np.random.randint(0, train_images.shape[0], 9)\n",
    "_ = isns.ImageGrid([train_images[i, :, :, :].reshape(28, 28) for i in ints],\n",
    "                   cbar_label=[f'{train_targets[i]}' for i in ints],\n",
    "                   col_wrap=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_targets = tf.keras.utils.to_categorical(train_targets, num_classes=10, dtype='float32')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 10)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def scale_and_augment(input):\n",
    "    x = Rescaling(1./255)(input)\n",
    "    x = RandomRotation(.042, fill_mode='constant')(x)\n",
    "    x = RandomTranslation(.25, .25, fill_mode='constant')(x)\n",
    "    x = RandomZoom((-.2, .3), fill_mode='constant')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def conv_block(input, depth, kernel_size, num_kernels, activation, batchnorm):\n",
    "    for _ in range(depth):\n",
    "        x = Conv2D(filters=num_kernels,\n",
    "                   kernel_size=kernel_size,\n",
    "                   padding='same',\n",
    "                   )(input)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(activation)(x)\n",
    "\n",
    "    return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def build_cnn_1(weights=None):\n",
    "    name = 'cnn_1'\n",
    "    inputs = Input(shape=train_images.shape[1:])\n",
    "\n",
    "    x = scale_and_augment(inputs)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=64,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=128,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=256,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss=CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def build_cnn_2(weights=None):\n",
    "    name = 'cnn_2'\n",
    "    inputs = Input(shape=train_images.shape[1:])\n",
    "\n",
    "    x = scale_and_augment(inputs)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=16,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=32,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=64,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=128,\n",
    "                   activation='relu',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss=CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def build_cnn_3(weights=None):\n",
    "    name = 'cnn_3'\n",
    "    inputs = Input(shape=train_images.shape[1:])\n",
    "\n",
    "    x = scale_and_augment(inputs)\n",
    "\n",
    "    x1 = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=32,\n",
    "                   activation='swish',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x2 = conv_block(input=x,\n",
    "               depth=2,\n",
    "               kernel_size=5,\n",
    "               num_kernels=32,\n",
    "               activation='swish',\n",
    "               batchnorm=True)\n",
    "\n",
    "    x2 = conv_block(input=x2,\n",
    "               depth=2,\n",
    "               kernel_size=3,\n",
    "               num_kernels=32,\n",
    "               activation='swish',\n",
    "               batchnorm=True)\n",
    "\n",
    "    x3 = conv_block(input=x,\n",
    "               depth=1,\n",
    "               kernel_size=7,\n",
    "               num_kernels=32,\n",
    "               activation='swish',\n",
    "               batchnorm=True)\n",
    "\n",
    "\n",
    "    x = Concatenate()([x1, x2, x3])\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.2)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=128,\n",
    "                   activation='swish',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.2)(x)\n",
    "\n",
    "    x = conv_block(input=x,\n",
    "                   depth=3,\n",
    "                   kernel_size=3,\n",
    "                   num_kernels=256,\n",
    "                   activation='swish',\n",
    "                   batchnorm=True)\n",
    "\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    x = Dropout(.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='swish')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss=CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model, name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "model, name = build_cnn_3()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_1 (Rescaling)        (None, 28, 28, 1)    0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " random_rotation_1 (RandomRotat  (None, 28, 28, 1)   0           ['rescaling_1[0][0]']            \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " random_translation_1 (RandomTr  (None, 28, 28, 1)   0           ['random_rotation_1[0][0]']      \n",
      " anslation)                                                                                       \n",
      "                                                                                                  \n",
      " random_zoom_1 (RandomZoom)     (None, 28, 28, 1)    0           ['random_translation_1[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 28, 28, 32)   832         ['random_zoom_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 28, 28, 32)  128         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 28, 28, 32)   320         ['random_zoom_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 28, 28, 32)   9248        ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 28, 28, 32)   1600        ['random_zoom_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 28, 28, 32)  128         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 28, 28, 32)  128         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 28, 28, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 28, 28, 96)   0           ['activation_16[0][0]',          \n",
      "                                                                  'activation_20[0][0]',          \n",
      "                                                                  'activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 96)  0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 14, 14, 96)   0           ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 14, 14, 128)  110720      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 14, 14, 128)  512        ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 14, 14, 128)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 7, 7, 128)   0           ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 7, 7, 128)    0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 7, 7, 256)    295168      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 7, 7, 256)   1024        ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 7, 7, 256)    0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 3, 3, 256)   0           ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 3, 3, 256)    0           ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2304)         0           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256)          590080      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 256)         1024        ['dense_2[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 10)           2570        ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,013,610\n",
      "Trainable params: 1,012,074\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((train_images, train_targets))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def brightness_augment(image, target):\n",
    "    image = tf.math.multiply(image, tf.random.uniform(shape=[], minval=.9, maxval=1))\n",
    "\n",
    "    return image, target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "SPLIT = .15\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "ds_eval = ds.take(round(train_targets.shape[0]*SPLIT))\n",
    "ds_train = ds.skip(round(train_targets.shape[0]*SPLIT))\n",
    "\n",
    "# ds_train = ds_train.map(brightness_augment)\n",
    "\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_eval = ds_eval.batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_train = ds_train.cache().shuffle(2000).prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.0242 - accuracy: 0.6696\n",
      "Epoch 1: val_accuracy improved from -inf to 0.17978, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 9s 151ms/step - loss: 1.0242 - accuracy: 0.6696 - val_loss: 2.3891 - val_accuracy: 0.1798\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.9035\n",
      "Epoch 2: val_accuracy did not improve from 0.17978\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.2993 - accuracy: 0.9035 - val_loss: 2.7483 - val_accuracy: 0.1000\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9402\n",
      "Epoch 3: val_accuracy did not improve from 0.17978\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.1851 - accuracy: 0.9402 - val_loss: 3.0071 - val_accuracy: 0.1093\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9513\n",
      "Epoch 4: val_accuracy did not improve from 0.17978\n",
      "25/25 [==============================] - 3s 141ms/step - loss: 0.1488 - accuracy: 0.9513 - val_loss: 3.2605 - val_accuracy: 0.1457\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9616\n",
      "Epoch 5: val_accuracy did not improve from 0.17978\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.1205 - accuracy: 0.9616 - val_loss: 3.3964 - val_accuracy: 0.1640\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9648\n",
      "Epoch 6: val_accuracy did not improve from 0.17978\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.1103 - accuracy: 0.9648 - val_loss: 3.5166 - val_accuracy: 0.1548\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9680\n",
      "Epoch 7: val_accuracy improved from 0.17978 to 0.18767, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0976 - accuracy: 0.9680 - val_loss: 3.5379 - val_accuracy: 0.1877\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9727\n",
      "Epoch 8: val_accuracy did not improve from 0.18767\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0858 - accuracy: 0.9727 - val_loss: 3.6591 - val_accuracy: 0.1817\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9731\n",
      "Epoch 9: val_accuracy did not improve from 0.18767\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0800 - accuracy: 0.9731 - val_loss: 3.9044 - val_accuracy: 0.1306\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9747\n",
      "Epoch 10: val_accuracy did not improve from 0.18767\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0762 - accuracy: 0.9747 - val_loss: 3.8384 - val_accuracy: 0.1098\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9768\n",
      "Epoch 11: val_accuracy did not improve from 0.18767\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0704 - accuracy: 0.9768 - val_loss: 3.5402 - val_accuracy: 0.1082\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9776\n",
      "Epoch 12: val_accuracy did not improve from 0.18767\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0687 - accuracy: 0.9776 - val_loss: 3.1460 - val_accuracy: 0.1667\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9791\n",
      "Epoch 13: val_accuracy did not improve from 0.18767\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.0630 - accuracy: 0.9791 - val_loss: 5.2090 - val_accuracy: 0.1019\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9801\n",
      "Epoch 14: val_accuracy improved from 0.18767 to 0.52133, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 0.0599 - accuracy: 0.9801 - val_loss: 1.3693 - val_accuracy: 0.5213\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9797\n",
      "Epoch 15: val_accuracy improved from 0.52133 to 0.75856, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0614 - accuracy: 0.9797 - val_loss: 0.7118 - val_accuracy: 0.7586\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9811\n",
      "Epoch 16: val_accuracy did not improve from 0.75856\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0573 - accuracy: 0.9811 - val_loss: 0.9096 - val_accuracy: 0.6970\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9825\n",
      "Epoch 17: val_accuracy did not improve from 0.75856\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0538 - accuracy: 0.9825 - val_loss: 0.8966 - val_accuracy: 0.7172\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9811\n",
      "Epoch 18: val_accuracy improved from 0.75856 to 0.92811, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.0567 - accuracy: 0.9811 - val_loss: 0.2145 - val_accuracy: 0.9281\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9826\n",
      "Epoch 19: val_accuracy did not improve from 0.92811\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0528 - accuracy: 0.9826 - val_loss: 0.2123 - val_accuracy: 0.9251\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9832\n",
      "Epoch 20: val_accuracy improved from 0.92811 to 0.97100, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0498 - accuracy: 0.9832 - val_loss: 0.0898 - val_accuracy: 0.9710\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9835\n",
      "Epoch 21: val_accuracy improved from 0.97100 to 0.97489, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0483 - accuracy: 0.9835 - val_loss: 0.0716 - val_accuracy: 0.9749\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9840\n",
      "Epoch 22: val_accuracy improved from 0.97489 to 0.97867, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0469 - accuracy: 0.9840 - val_loss: 0.0702 - val_accuracy: 0.9787\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9853\n",
      "Epoch 23: val_accuracy improved from 0.97867 to 0.98644, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0442 - accuracy: 0.9853 - val_loss: 0.0473 - val_accuracy: 0.9864\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9847\n",
      "Epoch 24: val_accuracy improved from 0.98644 to 0.98700, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 0.0460 - accuracy: 0.9847 - val_loss: 0.0399 - val_accuracy: 0.9870\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9851\n",
      "Epoch 25: val_accuracy did not improve from 0.98700\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0454 - accuracy: 0.9851 - val_loss: 0.0623 - val_accuracy: 0.9788\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9855\n",
      "Epoch 26: val_accuracy improved from 0.98700 to 0.98733, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 135ms/step - loss: 0.0435 - accuracy: 0.9855 - val_loss: 0.0521 - val_accuracy: 0.9873\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9869\n",
      "Epoch 27: val_accuracy did not improve from 0.98733\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0403 - accuracy: 0.9869 - val_loss: 0.0452 - val_accuracy: 0.9872\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9854\n",
      "Epoch 28: val_accuracy did not improve from 0.98733\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0429 - accuracy: 0.9854 - val_loss: 0.0440 - val_accuracy: 0.9872\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9857\n",
      "Epoch 29: val_accuracy improved from 0.98733 to 0.99056, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0422 - accuracy: 0.9857 - val_loss: 0.0336 - val_accuracy: 0.9906\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9864\n",
      "Epoch 30: val_accuracy did not improve from 0.99056\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.0411 - val_accuracy: 0.9902\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9852\n",
      "Epoch 31: val_accuracy did not improve from 0.99056\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.0426 - accuracy: 0.9852 - val_loss: 0.0394 - val_accuracy: 0.9886\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9863\n",
      "Epoch 32: val_accuracy did not improve from 0.99056\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0403 - accuracy: 0.9863 - val_loss: 0.0422 - val_accuracy: 0.9878\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9873\n",
      "Epoch 33: val_accuracy did not improve from 0.99056\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0378 - accuracy: 0.9873 - val_loss: 0.0402 - val_accuracy: 0.9900\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9877\n",
      "Epoch 34: val_accuracy improved from 0.99056 to 0.99111, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0372 - accuracy: 0.9877 - val_loss: 0.0372 - val_accuracy: 0.9911\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9866\n",
      "Epoch 35: val_accuracy did not improve from 0.99111\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 0.0457 - val_accuracy: 0.9891\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9885\n",
      "Epoch 36: val_accuracy did not improve from 0.99111\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0351 - accuracy: 0.9885 - val_loss: 0.0388 - val_accuracy: 0.9910\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9878\n",
      "Epoch 37: val_accuracy did not improve from 0.99111\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0358 - accuracy: 0.9878 - val_loss: 0.0364 - val_accuracy: 0.9910\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9882\n",
      "Epoch 38: val_accuracy did not improve from 0.99111\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0353 - accuracy: 0.9882 - val_loss: 0.0462 - val_accuracy: 0.9881\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9878\n",
      "Epoch 39: val_accuracy improved from 0.99111 to 0.99133, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0350 - accuracy: 0.9878 - val_loss: 0.0322 - val_accuracy: 0.9913\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9886\n",
      "Epoch 40: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0343 - accuracy: 0.9886 - val_loss: 0.0394 - val_accuracy: 0.9889\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9881\n",
      "Epoch 41: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0342 - accuracy: 0.9881 - val_loss: 0.0432 - val_accuracy: 0.9886\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9886\n",
      "Epoch 42: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0338 - accuracy: 0.9886 - val_loss: 0.0407 - val_accuracy: 0.9906\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9888\n",
      "Epoch 43: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0338 - accuracy: 0.9888 - val_loss: 0.0433 - val_accuracy: 0.9888\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9889\n",
      "Epoch 44: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0333 - accuracy: 0.9889 - val_loss: 0.0449 - val_accuracy: 0.9881\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9889\n",
      "Epoch 45: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0329 - accuracy: 0.9889 - val_loss: 0.0400 - val_accuracy: 0.9901\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9893\n",
      "Epoch 46: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0312 - accuracy: 0.9893 - val_loss: 0.0453 - val_accuracy: 0.9877\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9891\n",
      "Epoch 47: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0322 - accuracy: 0.9891 - val_loss: 0.0465 - val_accuracy: 0.9874\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9890\n",
      "Epoch 48: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0322 - accuracy: 0.9890 - val_loss: 0.0412 - val_accuracy: 0.9887\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9886\n",
      "Epoch 49: val_accuracy did not improve from 0.99133\n",
      "25/25 [==============================] - 3s 139ms/step - loss: 0.0333 - accuracy: 0.9886 - val_loss: 0.0476 - val_accuracy: 0.9889\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9886\n",
      "Epoch 50: val_accuracy improved from 0.99133 to 0.99311, saving model to ./models/cnn_3\\weights_best.h5\n",
      "25/25 [==============================] - 3s 134ms/step - loss: 0.0332 - accuracy: 0.9886 - val_loss: 0.0321 - val_accuracy: 0.9931\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9897\n",
      "Epoch 51: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0311 - accuracy: 0.9897 - val_loss: 0.0328 - val_accuracy: 0.9912\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9900\n",
      "Epoch 52: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 0.0351 - val_accuracy: 0.9904\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9899\n",
      "Epoch 53: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.0312 - accuracy: 0.9899 - val_loss: 0.0334 - val_accuracy: 0.9917\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9903\n",
      "Epoch 54: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.0283 - accuracy: 0.9903 - val_loss: 0.0355 - val_accuracy: 0.9911\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9904\n",
      "Epoch 55: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0287 - accuracy: 0.9904 - val_loss: 0.0456 - val_accuracy: 0.9889\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9895\n",
      "Epoch 56: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0307 - accuracy: 0.9895 - val_loss: 0.0390 - val_accuracy: 0.9896\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9904\n",
      "Epoch 57: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0285 - accuracy: 0.9904 - val_loss: 0.0484 - val_accuracy: 0.9857\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9910\n",
      "Epoch 58: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0276 - accuracy: 0.9910 - val_loss: 0.0456 - val_accuracy: 0.9859\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9908\n",
      "Epoch 59: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.0409 - val_accuracy: 0.9896\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9904\n",
      "Epoch 60: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0276 - accuracy: 0.9904 - val_loss: 0.0352 - val_accuracy: 0.9912\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9901\n",
      "Epoch 61: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0286 - accuracy: 0.9901 - val_loss: 0.0411 - val_accuracy: 0.9900\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9903\n",
      "Epoch 62: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0289 - accuracy: 0.9903 - val_loss: 0.0366 - val_accuracy: 0.9919\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9909\n",
      "Epoch 63: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0266 - accuracy: 0.9909 - val_loss: 0.0373 - val_accuracy: 0.9917\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9910\n",
      "Epoch 64: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0261 - accuracy: 0.9910 - val_loss: 0.0406 - val_accuracy: 0.9877\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9921\n",
      "Epoch 65: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0258 - accuracy: 0.9921 - val_loss: 0.0406 - val_accuracy: 0.9902\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9914\n",
      "Epoch 66: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.0256 - accuracy: 0.9914 - val_loss: 0.0403 - val_accuracy: 0.9874\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9910\n",
      "Epoch 67: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0262 - accuracy: 0.9910 - val_loss: 0.0392 - val_accuracy: 0.9902\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9906\n",
      "Epoch 68: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0268 - accuracy: 0.9906 - val_loss: 0.0408 - val_accuracy: 0.9887\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9912\n",
      "Epoch 69: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0270 - accuracy: 0.9912 - val_loss: 0.0336 - val_accuracy: 0.9904\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9909\n",
      "Epoch 70: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0278 - accuracy: 0.9909 - val_loss: 0.0393 - val_accuracy: 0.9904\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9899\n",
      "Epoch 71: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0280 - accuracy: 0.9899 - val_loss: 0.0474 - val_accuracy: 0.9863\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9906\n",
      "Epoch 72: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0269 - accuracy: 0.9906 - val_loss: 0.0331 - val_accuracy: 0.9908\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9911\n",
      "Epoch 73: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.0423 - val_accuracy: 0.9869\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9911\n",
      "Epoch 74: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0260 - accuracy: 0.9911 - val_loss: 0.0334 - val_accuracy: 0.9912\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9918\n",
      "Epoch 75: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0234 - accuracy: 0.9918 - val_loss: 0.0382 - val_accuracy: 0.9918\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9913\n",
      "Epoch 76: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0263 - accuracy: 0.9913 - val_loss: 0.0299 - val_accuracy: 0.9924\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9906\n",
      "Epoch 77: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0260 - accuracy: 0.9906 - val_loss: 0.0317 - val_accuracy: 0.9924\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9922\n",
      "Epoch 78: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0251 - accuracy: 0.9922 - val_loss: 0.0410 - val_accuracy: 0.9910\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9919\n",
      "Epoch 79: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0275 - val_accuracy: 0.9927\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9923\n",
      "Epoch 80: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0331 - val_accuracy: 0.9904\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9917\n",
      "Epoch 81: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0240 - accuracy: 0.9917 - val_loss: 0.0339 - val_accuracy: 0.9917\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9925\n",
      "Epoch 82: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.0489 - val_accuracy: 0.9878\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9915\n",
      "Epoch 83: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0247 - accuracy: 0.9915 - val_loss: 0.0331 - val_accuracy: 0.9910\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9921\n",
      "Epoch 84: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0243 - accuracy: 0.9921 - val_loss: 0.0320 - val_accuracy: 0.9916\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9920\n",
      "Epoch 85: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0220 - accuracy: 0.9920 - val_loss: 0.0366 - val_accuracy: 0.9918\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9919\n",
      "Epoch 86: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0240 - accuracy: 0.9919 - val_loss: 0.0348 - val_accuracy: 0.9911\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9925\n",
      "Epoch 87: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0224 - accuracy: 0.9925 - val_loss: 0.0476 - val_accuracy: 0.9898\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9914\n",
      "Epoch 88: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0242 - accuracy: 0.9914 - val_loss: 0.0431 - val_accuracy: 0.9881\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9924\n",
      "Epoch 89: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.0528 - val_accuracy: 0.9847\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9919\n",
      "Epoch 90: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0241 - accuracy: 0.9919 - val_loss: 0.0449 - val_accuracy: 0.9888\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9918\n",
      "Epoch 91: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 0.0397 - val_accuracy: 0.9903\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9919\n",
      "Epoch 92: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0232 - accuracy: 0.9919 - val_loss: 0.0398 - val_accuracy: 0.9908\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9922\n",
      "Epoch 93: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0229 - accuracy: 0.9922 - val_loss: 0.0377 - val_accuracy: 0.9913\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9916\n",
      "Epoch 94: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0244 - accuracy: 0.9916 - val_loss: 0.0467 - val_accuracy: 0.9890\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9918\n",
      "Epoch 95: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.0242 - accuracy: 0.9918 - val_loss: 0.0350 - val_accuracy: 0.9913\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9924\n",
      "Epoch 96: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0415 - val_accuracy: 0.9890\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9916\n",
      "Epoch 97: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 0.0237 - accuracy: 0.9916 - val_loss: 0.0342 - val_accuracy: 0.9914\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9919\n",
      "Epoch 98: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0247 - accuracy: 0.9919 - val_loss: 0.0344 - val_accuracy: 0.9911\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9926\n",
      "Epoch 99: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 126ms/step - loss: 0.0227 - accuracy: 0.9926 - val_loss: 0.0347 - val_accuracy: 0.9910\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9924\n",
      "Epoch 100: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0215 - accuracy: 0.9924 - val_loss: 0.0380 - val_accuracy: 0.9890\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9924\n",
      "Epoch 101: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 0.0212 - accuracy: 0.9924 - val_loss: 0.0410 - val_accuracy: 0.9891\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9925\n",
      "Epoch 102: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0215 - accuracy: 0.9925 - val_loss: 0.0422 - val_accuracy: 0.9902\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9928\n",
      "Epoch 103: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 129ms/step - loss: 0.0216 - accuracy: 0.9928 - val_loss: 0.0442 - val_accuracy: 0.9871\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9922\n",
      "Epoch 104: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 131ms/step - loss: 0.0220 - accuracy: 0.9922 - val_loss: 0.0317 - val_accuracy: 0.9923\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9928\n",
      "Epoch 105: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 0.0225 - accuracy: 0.9928 - val_loss: 0.0365 - val_accuracy: 0.9912\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9929\n",
      "Epoch 106: val_accuracy did not improve from 0.99311\n",
      "25/25 [==============================] - 3s 130ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.0360 - val_accuracy: 0.9914\n",
      "Epoch 107/500\n",
      "23/25 [==========================>...] - ETA: 0s - loss: 0.0200 - accuracy: 0.9932"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mds_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mds_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mModelCheckpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./models/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/weights_best.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mval_accuracy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_best_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\engine\\training.py:1389\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1387\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs  \u001B[38;5;66;03m# No error, now safe to assign to logs.\u001B[39;00m\n\u001B[0;32m   1388\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[1;32m-> 1389\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[0;32m   1391\u001B[0m   \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\callbacks.py:438\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[0;32m    432\u001B[0m \n\u001B[0;32m    433\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[0;32m    436\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[1;32m--> 438\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\callbacks.py:297\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    295\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 297\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    299\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    300\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Expected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\callbacks.py:318\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    315\u001B[0m   batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[0;32m    316\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[1;32m--> 318\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[0;32m    321\u001B[0m   end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\callbacks.py:356\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m    355\u001B[0m   hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[1;32m--> 356\u001B[0m   \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[0;32m    359\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\callbacks.py:1034\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m-> 1034\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\callbacks.py:1106\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1102\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m add_seen\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1105\u001B[0m   \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[1;32m-> 1106\u001B[0m   logs \u001B[38;5;241m=\u001B[39m \u001B[43mtf_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync_to_numpy_or_python_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1107\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen, \u001B[38;5;28mlist\u001B[39m(logs\u001B[38;5;241m.\u001B[39mitems()), finalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\tf_utils.py:563\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    560\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n\u001B[0;32m    561\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(t) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m--> 563\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_single_numpy_or_python_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001B[0m, in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    910\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    911\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 914\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    915\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    910\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    911\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 914\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    915\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\tf_utils.py:557\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[0;32m    555\u001B[0m   \u001B[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[0;32m    556\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, tf\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 557\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    558\u001B[0m   \u001B[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001B[39;00m\n\u001B[0;32m    559\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mgeneric)):\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py:1223\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[0;32m   1201\u001B[0m \n\u001B[0;32m   1202\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[0;32m   1221\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1222\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[1;32m-> 1223\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py:1189\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1188\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1190\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "                    ds_train,\n",
    "                    validation_data=ds_eval,\n",
    "                    epochs=150,\n",
    "                    callbacks=[ModelCheckpoint(f'./models/{name}/weights_best.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)]\n",
    "                    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], 'g')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.title(f'{name} Loss across epochs\\n')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('# epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_weights(f'./models/{name}/weights_best.h5')\n",
    "\n",
    "scores = model.evaluate(ds_eval)\n",
    "\n",
    "os.makedirs(f'./models/{name}/model', exist_ok=True)\n",
    "model.save(filepath=f'./models/{name}/model', save_format='tf')\n",
    "\n",
    "print(\"done training!\")\n",
    "\n",
    "original_stdout = sys.stdout\n",
    "with open(f'./models/{name}/training.info', 'w') as f:\n",
    "    sys.stdout = f\n",
    "\n",
    "    print(f'\\n\\n***MODEL SUMMARY***\\nacc: {scores[2]}')\n",
    "    print(model.summary())\n",
    "    print('\\n\\n****FLOPS***\\n')\n",
    "\n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "print('done experiment!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Ensemble + Pseudo Labelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "test_data_csv = pd.read_csv('data/test.csv')\n",
    "test_images = train_featrues.values.astype('float32').reshape(-1, 28, 28, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "dig_data_csv = pd.read_csv('data/Dig-MNIST.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7   \n0          0       0       0       0       0       0       0       0       0  \\\n1          1       0       0       0       0       0       0       0       0   \n2          2       0       0       0       0       0       0       0       0   \n3          3       0       0       0       0       0       0       0       0   \n4          4       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n10235      5       0       0       0       0       0       0       0       0   \n10236      6       0       0       0       0       0       0       0       0   \n10237      7       0       0       0       0       0       0       0       0   \n10238      8       0       0       0       0       0       0       0       0   \n10239      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778   \n0           0  ...         0         0         0         0         0  \\\n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n10235       0  ...         0         0         0         0         0   \n10236       0  ...         0         0         0         0         0   \n10237       0  ...         0         0         0         0         0   \n10238       0  ...         0         0         0         0         0   \n10239       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n10235         0         0         0         0         0  \n10236         0         0         0         0         0  \n10237         0         0         0         0         0  \n10238         0         0         0         0         0  \n10239         0         0         0         0         0  \n\n[10240 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10235</th>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10236</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10237</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10238</th>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10239</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10240 rows × 785 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig_data_csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(3):\n",
    "    ds = ds.shuffle(len(ds))\n",
    "\n",
    "    SPLIT = .15\n",
    "    BATCH_SIZE = 2048\n",
    "\n",
    "    ds_eval = ds.take(round(train_targets.shape[0] * SPLIT))\n",
    "    ds_train = ds.skip(round(train_targets.shape[0] * SPLIT))\n",
    "\n",
    "    ds_train = ds_train.map(brightness_augment)\n",
    "\n",
    "    ds_train = ds_train.batch(BATCH_SIZE)\n",
    "    ds_eval = ds_eval.batch(BATCH_SIZE)\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    ds_train = ds_train.cache().shuffle(2000).prefetch(buffer_size=AUTOTUNE)\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_eval,\n",
    "        epochs=150,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(f'./models/{name}/weights_best.h5', monitor='val_accuracy', save_best_only=True, mode='max',\n",
    "                            verbose=1)]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
